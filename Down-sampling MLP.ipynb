{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    6178\n",
       "2    6178\n",
       "1    6178\n",
       "0    6178\n",
       "Name: CRASH_SEV_CODE, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "df = pd.read_csv(\"crash_data_median_clustered.csv\")\n",
    "df2 = pd.read_csv(\"crash_data_only_numeric_values.csv\")\n",
    "\n",
    "#add crash_sev_code to one_hot_encoded\n",
    "df = df.assign(CRASH_SEV_CODE = df2[\"CRASH_SEV_CODE\"])\n",
    "df.drop('CRASH_SEV_F', axis='columns', inplace=True)\n",
    "df.drop('CRASH_SEV_M', axis='columns', inplace=True)\n",
    "df.drop('CRASH_SEV_N', axis='columns', inplace=True)\n",
    "df.drop('CRASH_SEV_S', axis='columns', inplace=True)\n",
    "#for cluster_scaled\n",
    "df.drop('CLUSTER', axis='columns', inplace=True)\n",
    "#for cluster\n",
    "#df.drop('CLUSTER_SCALED', axis='columns', inplace=True)\n",
    "\n",
    "df = sklearn.utils.shuffle(df)\n",
    "df.fillna(0,inplace=True)\n",
    "encode = LabelEncoder()\n",
    "\n",
    "features = ['CRASH_YEAR', \n",
    "'NUM_LANES', \n",
    "'SPD_LIM', \n",
    "'TRAFFIC_CTRL_Give Way Sign', \n",
    "'TRAFFIC_CTRL_Nil', \n",
    "'TRAFFIC_CTRL_Points Man', \n",
    "'TRAFFIC_CTRL_School Patrol', \n",
    "'TRAFFIC_CTRL_Stop Sign', \n",
    "'TRAFFIC_CTRL_Traffic Signal', \n",
    "'MULTI_VEH_Cyclist(s)+Pedestrian(s) only', \n",
    "'MULTI_VEH_Cyclists only', \n",
    "'MULTI_VEH_Multi vehicle', \n",
    "'MULTI_VEH_Other', \n",
    "'MULTI_VEH_Others without non-parked veh', \n",
    "'MULTI_VEH_Single vehicle', \n",
    "'MULTI_VEH_Vehicle(s)+Cyclist(s) only', \n",
    "'MULTI_VEH_Vehicle(s)+Pedestrian(s)', \n",
    "'MULTI_VEH_Vehicle(s)+multiple other types', \n",
    "'HOLIDAY_Christmas/New Year', \n",
    "'HOLIDAY_Easter', \n",
    "'HOLIDAY_Labour Weekend', \n",
    "'HOLIDAY_None', \n",
    "'HOLIDAY_Queens Birthday', \n",
    "'LG_REGION_DESC_0', \n",
    "'LG_REGION_DESC_Auckland            ', \n",
    "'LG_REGION_DESC_Bay of Plenty       ', \n",
    "'LG_REGION_DESC_Canterbury          ', \n",
    "'LG_REGION_DESC_Gisborne            ', \n",
    "'LG_REGION_DESC_Hawkes Bay          ', \n",
    "'LG_REGION_DESC_Manawatu/Wanganui   ', \n",
    "'LG_REGION_DESC_Nelson/Marlborough  ', \n",
    "'LG_REGION_DESC_Northland           ', \n",
    "'LG_REGION_DESC_Otago               ', \n",
    "'LG_REGION_DESC_Southland           ', \n",
    "'LG_REGION_DESC_Taranaki            ', \n",
    "'LG_REGION_DESC_Waikato             ', \n",
    "'LG_REGION_DESC_Wellington          ', \n",
    "'LG_REGION_DESC_West Coast          ', \n",
    "'JUNCTION_TYPE_Driveway', \n",
    "'JUNCTION_TYPE_Multi Rd Join', \n",
    "'JUNCTION_TYPE_Roundabout', \n",
    "'JUNCTION_TYPE_T Type Junction',  \n",
    "'JUNCTION_TYPE_X Type Junction', \n",
    "'JUNCTION_TYPE_Y Type Junction', \n",
    "'DIRN_ROLE1_DESC_0', \n",
    "'DIRN_ROLE1_DESC_East', \n",
    "'DIRN_ROLE1_DESC_North', \n",
    "'DIRN_ROLE1_DESC_South',  \n",
    "'DIRN_ROLE1_DESC_West', \n",
    "'INTSN_MIDBLOCK_Intersection', \n",
    "'INTSN_MIDBLOCK_Mid Block', \n",
    "'FLAT_HILL_Flat', \n",
    "'FLAT_HILL_Hill',  \n",
    "'ROAD_CURVATURE_Easy Curve', \n",
    "'ROAD_CURVATURE_Moderate Curve', \n",
    "'ROAD_CURVATURE_Severe Curve', \n",
    "'ROAD_CURVATURE_Straight Road',  \n",
    "'ROAD_MARKINGS_Centre Line', \n",
    "'ROAD_MARKINGS_No Marks', \n",
    "'ROAD_MARKINGS_No Passing Lines', \n",
    "'ROAD_MARKINGS_Painted Island', \n",
    "'ROAD_MARKINGS_Ped Crossing', \n",
    "'ROAD_MARKINGS_Raised Island', \n",
    "'ROAD_SURFACE_Sealed', \n",
    "'ROAD_SURFACE_Unsealed', \n",
    "'ROAD_WET_Dry', \n",
    "'ROAD_WET_Ice/ Snow', \n",
    "'ROAD_WET_Wet', \n",
    "'URBAN_Openroad', \n",
    "'URBAN_Urban', \n",
    "'LIGHT_Bright Sun', \n",
    "'LIGHT_Dark', \n",
    "'LIGHT_Overcast', \n",
    "'LIGHT_Twilight',  \n",
    "'STREET_LIGHT_None', \n",
    "'STREET_LIGHT_Off', \n",
    "'STREET_LIGHT_On',  \n",
    "'WEATHER_A_Fine', \n",
    "'WEATHER_A_Heavy Rain', \n",
    "'WEATHER_A_Light Rain', \n",
    "'WEATHER_A_Mist', \n",
    "'WEATHER_A_Snow',\n",
    "'CLUSTER_SCALED']\n",
    "\n",
    "df_non_injury = df[df.CRASH_SEV_CODE==2]  # 480452 samples\n",
    "df_minor = df[df.CRASH_SEV_CODE==1]       # 150834 samples\n",
    "df_serious = df[df.CRASH_SEV_CODE==3]    # 37347  samples\n",
    "df_fatal = df[df.CRASH_SEV_CODE==0]       # 6178   samples\n",
    "\n",
    "df_minor_downsampled = resample(df_minor, replace=True,n_samples=6178,random_state=40)\n",
    "df_serious_downsampled = resample(df_serious, replace=True,n_samples=6178,random_state=40)\n",
    "df_non_injury_downsampled = resample(df_non_injury, replace=True,n_samples=6178,random_state=40)\n",
    "\n",
    "df_downsampled = pd.concat([df_non_injury_downsampled, df_minor_downsampled, df_serious_downsampled, df_fatal])\n",
    "\n",
    "df_downsampled.CRASH_SEV_CODE.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22240, 83)\n",
      "(2472, 83)\n"
     ]
    }
   ],
   "source": [
    "X = df_downsampled.drop([\"CRASH_SEV_CODE\"], axis=1).values\n",
    "y = df_downsampled[\"CRASH_SEV_CODE\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=40)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.68853213\n",
      "Iteration 2, loss = 1.56403993\n",
      "Iteration 3, loss = 1.46016418\n",
      "Iteration 4, loss = 1.37637323\n",
      "Iteration 5, loss = 1.32336755\n",
      "Iteration 6, loss = 1.29368402\n",
      "Iteration 7, loss = 1.27858927\n",
      "Iteration 8, loss = 1.26833813\n",
      "Iteration 9, loss = 1.26125493\n",
      "Iteration 10, loss = 1.25663255\n",
      "Iteration 11, loss = 1.25078756\n",
      "Iteration 12, loss = 1.24566724\n",
      "Iteration 13, loss = 1.23883059\n",
      "Iteration 14, loss = 1.23343436\n",
      "Iteration 15, loss = 1.22834648\n",
      "Iteration 16, loss = 1.22337388\n",
      "Iteration 17, loss = 1.21775712\n",
      "Iteration 18, loss = 1.21358792\n",
      "Iteration 19, loss = 1.20962781\n",
      "Iteration 20, loss = 1.20717132\n",
      "Iteration 21, loss = 1.20355253\n",
      "Iteration 22, loss = 1.20284664\n",
      "Iteration 23, loss = 1.19876037\n",
      "Iteration 24, loss = 1.19677351\n",
      "Iteration 25, loss = 1.19365519\n",
      "Iteration 26, loss = 1.19218480\n",
      "Iteration 27, loss = 1.18963672\n",
      "Iteration 28, loss = 1.18799120\n",
      "Iteration 29, loss = 1.18522806\n",
      "Iteration 30, loss = 1.18424683\n",
      "Iteration 31, loss = 1.18313866\n",
      "Iteration 32, loss = 1.18180570\n",
      "Iteration 33, loss = 1.17931928\n",
      "Iteration 34, loss = 1.17791824\n",
      "Iteration 35, loss = 1.17706889\n",
      "Iteration 36, loss = 1.17564890\n",
      "Iteration 37, loss = 1.17368028\n",
      "Iteration 38, loss = 1.17133232\n",
      "Iteration 39, loss = 1.17075262\n",
      "Iteration 40, loss = 1.17016938\n",
      "Iteration 41, loss = 1.16829195\n",
      "Iteration 42, loss = 1.16646242\n",
      "Iteration 43, loss = 1.16570361\n",
      "Iteration 44, loss = 1.16314284\n",
      "Iteration 45, loss = 1.16303514\n",
      "Iteration 46, loss = 1.16280957\n",
      "Iteration 47, loss = 1.16151522\n",
      "Iteration 48, loss = 1.16117039\n",
      "Iteration 49, loss = 1.15902581\n",
      "Iteration 50, loss = 1.15807746\n",
      "Iteration 51, loss = 1.15759150\n",
      "Iteration 52, loss = 1.15515603\n",
      "Iteration 53, loss = 1.15518576\n",
      "Iteration 54, loss = 1.15400467\n",
      "Iteration 55, loss = 1.15425186\n",
      "Iteration 56, loss = 1.15272708\n",
      "Iteration 57, loss = 1.15181101\n",
      "Iteration 58, loss = 1.15075218\n",
      "Iteration 59, loss = 1.15026709\n",
      "Iteration 60, loss = 1.14943896\n",
      "Iteration 61, loss = 1.14960504\n",
      "Iteration 62, loss = 1.14995842\n",
      "Iteration 63, loss = 1.14881505\n",
      "Iteration 64, loss = 1.14773876\n",
      "Iteration 65, loss = 1.14650912\n",
      "Iteration 66, loss = 1.14676000\n",
      "Iteration 67, loss = 1.14443458\n",
      "Iteration 68, loss = 1.14377849\n",
      "Iteration 69, loss = 1.14362352\n",
      "Iteration 70, loss = 1.14321105\n",
      "Iteration 71, loss = 1.14145910\n",
      "Iteration 72, loss = 1.14090344\n",
      "Iteration 73, loss = 1.14164351\n",
      "Iteration 74, loss = 1.13941582\n",
      "Iteration 75, loss = 1.13815798\n",
      "Iteration 76, loss = 1.13940477\n",
      "Iteration 77, loss = 1.13839998\n",
      "Iteration 78, loss = 1.13743482\n",
      "Iteration 79, loss = 1.13771489\n",
      "Iteration 80, loss = 1.13612893\n",
      "Iteration 81, loss = 1.13562486\n",
      "Iteration 82, loss = 1.13615858\n",
      "Iteration 83, loss = 1.13633584\n",
      "Iteration 84, loss = 1.13552281\n",
      "Iteration 85, loss = 1.13465433\n",
      "Iteration 86, loss = 1.13445519\n",
      "Iteration 87, loss = 1.13499927\n",
      "Iteration 88, loss = 1.13294254\n",
      "Iteration 89, loss = 1.13162793\n",
      "Iteration 90, loss = 1.13147686\n",
      "Iteration 91, loss = 1.13254227\n",
      "Iteration 92, loss = 1.13159729\n",
      "Iteration 93, loss = 1.13205395\n",
      "Iteration 94, loss = 1.13051419\n",
      "Iteration 95, loss = 1.13055554\n",
      "Iteration 96, loss = 1.12781285\n",
      "Iteration 97, loss = 1.12892189\n",
      "Iteration 98, loss = 1.12841881\n",
      "Iteration 99, loss = 1.12778826\n",
      "Iteration 100, loss = 1.12740798\n",
      "Iteration 101, loss = 1.12626588\n",
      "Iteration 102, loss = 1.12642738\n",
      "Iteration 103, loss = 1.12560469\n",
      "Iteration 104, loss = 1.12511799\n",
      "Iteration 105, loss = 1.12725632\n",
      "Iteration 106, loss = 1.12448255\n",
      "Iteration 107, loss = 1.12315187\n",
      "Iteration 108, loss = 1.12447135\n",
      "Iteration 109, loss = 1.12354087\n",
      "Iteration 110, loss = 1.12366559\n",
      "Iteration 111, loss = 1.12349131\n",
      "Iteration 112, loss = 1.12416989\n",
      "Iteration 113, loss = 1.12278471\n",
      "Iteration 114, loss = 1.12206210\n",
      "Iteration 115, loss = 1.12112381\n",
      "Iteration 116, loss = 1.12215822\n",
      "Iteration 117, loss = 1.12229279\n",
      "Iteration 118, loss = 1.12046651\n",
      "Iteration 119, loss = 1.11978687\n",
      "Iteration 120, loss = 1.11888942\n",
      "Iteration 121, loss = 1.11887981\n",
      "Iteration 122, loss = 1.11903576\n",
      "Iteration 123, loss = 1.11939730\n",
      "Iteration 124, loss = 1.11743404\n",
      "Iteration 125, loss = 1.11808801\n",
      "Iteration 126, loss = 1.11748707\n",
      "Iteration 127, loss = 1.11861447\n",
      "Iteration 128, loss = 1.11681348\n",
      "Iteration 129, loss = 1.11690282\n",
      "Iteration 130, loss = 1.11705488\n",
      "Iteration 131, loss = 1.11613570\n",
      "Iteration 132, loss = 1.11643342\n",
      "Iteration 133, loss = 1.11627885\n",
      "Iteration 134, loss = 1.11662782\n",
      "Iteration 135, loss = 1.11512713\n",
      "Iteration 136, loss = 1.11423835\n",
      "Iteration 137, loss = 1.11450151\n",
      "Iteration 138, loss = 1.11460615\n",
      "Iteration 139, loss = 1.11361850\n",
      "Iteration 140, loss = 1.11288279\n",
      "Iteration 141, loss = 1.11291629\n",
      "Iteration 142, loss = 1.11317213\n",
      "Iteration 143, loss = 1.11393655\n",
      "Iteration 144, loss = 1.11351539\n",
      "Iteration 145, loss = 1.11128078\n",
      "Iteration 146, loss = 1.11136428\n",
      "Iteration 147, loss = 1.11095226\n",
      "Iteration 148, loss = 1.11187355\n",
      "Iteration 149, loss = 1.11242273\n",
      "Iteration 150, loss = 1.11135937\n",
      "Iteration 151, loss = 1.10990646\n",
      "Iteration 152, loss = 1.10952441\n",
      "Iteration 153, loss = 1.11038402\n",
      "Iteration 154, loss = 1.10969615\n",
      "Iteration 155, loss = 1.10877123\n",
      "Iteration 156, loss = 1.11117214\n",
      "Iteration 157, loss = 1.10973647\n",
      "Iteration 158, loss = 1.10737840\n",
      "Iteration 159, loss = 1.10899194\n",
      "Iteration 160, loss = 1.10923355\n",
      "Iteration 161, loss = 1.10690254\n",
      "Iteration 162, loss = 1.10772443\n",
      "Iteration 163, loss = 1.10711399\n",
      "Iteration 164, loss = 1.10774914\n",
      "Iteration 165, loss = 1.10720447\n",
      "Iteration 166, loss = 1.10714527\n",
      "Iteration 167, loss = 1.10651014\n",
      "Iteration 168, loss = 1.10851883\n",
      "Iteration 169, loss = 1.10560565\n",
      "Iteration 170, loss = 1.10757440\n",
      "Iteration 171, loss = 1.10537767\n",
      "Iteration 172, loss = 1.10882911\n",
      "Iteration 173, loss = 1.10539313\n",
      "Iteration 174, loss = 1.10503003\n",
      "Iteration 175, loss = 1.10611573\n",
      "Iteration 176, loss = 1.10610366\n",
      "Iteration 177, loss = 1.10518511\n",
      "Iteration 178, loss = 1.10454468\n",
      "Iteration 179, loss = 1.10365704\n",
      "Iteration 180, loss = 1.10361643\n",
      "Iteration 181, loss = 1.10311269\n",
      "Iteration 182, loss = 1.10386274\n",
      "Iteration 183, loss = 1.10312238\n",
      "Iteration 184, loss = 1.10299054\n",
      "Iteration 185, loss = 1.10253190\n",
      "Iteration 186, loss = 1.10278565\n",
      "Iteration 187, loss = 1.10561914\n",
      "Iteration 188, loss = 1.10351286\n",
      "Iteration 189, loss = 1.10222991\n",
      "Iteration 190, loss = 1.10405958\n",
      "Iteration 191, loss = 1.10273110\n",
      "Iteration 192, loss = 1.10142485\n",
      "Iteration 193, loss = 1.10231096\n",
      "Iteration 194, loss = 1.10308379\n",
      "Iteration 195, loss = 1.10180251\n",
      "Iteration 196, loss = 1.10173715\n",
      "Iteration 197, loss = 1.10158105\n",
      "Iteration 198, loss = 1.10112259\n",
      "Iteration 199, loss = 1.10077297\n",
      "Iteration 200, loss = 1.10055554\n",
      "Iteration 201, loss = 1.10027491\n",
      "Iteration 202, loss = 1.09951682\n",
      "Iteration 203, loss = 1.09977059\n",
      "Iteration 204, loss = 1.09857980\n",
      "Iteration 205, loss = 1.09989562\n",
      "Iteration 206, loss = 1.10136671\n",
      "Iteration 207, loss = 1.10167917\n",
      "Iteration 208, loss = 1.10012226\n",
      "Iteration 209, loss = 1.10053118\n",
      "Iteration 210, loss = 1.09783508\n",
      "Iteration 211, loss = 1.09731926\n",
      "Iteration 212, loss = 1.09788979\n",
      "Iteration 213, loss = 1.09809335\n",
      "Iteration 214, loss = 1.09920125\n",
      "Iteration 215, loss = 1.09969950\n",
      "Iteration 216, loss = 1.09825247\n",
      "Iteration 217, loss = 1.09792363\n",
      "Iteration 218, loss = 1.09789481\n",
      "Iteration 219, loss = 1.09848318\n",
      "Iteration 220, loss = 1.09681930\n",
      "Iteration 221, loss = 1.09801317\n",
      "Iteration 222, loss = 1.09819897\n",
      "Iteration 223, loss = 1.09663151\n",
      "Iteration 224, loss = 1.09664677\n",
      "Iteration 225, loss = 1.09744864\n",
      "Iteration 226, loss = 1.09447197\n",
      "Iteration 227, loss = 1.09748055\n",
      "Iteration 228, loss = 1.09686386\n",
      "Iteration 229, loss = 1.09763540\n",
      "Iteration 230, loss = 1.09470221\n",
      "Iteration 231, loss = 1.09566537\n",
      "Iteration 232, loss = 1.09576472\n",
      "Iteration 233, loss = 1.09493996\n",
      "Iteration 234, loss = 1.09452297\n",
      "Iteration 235, loss = 1.09544068\n",
      "Iteration 236, loss = 1.09589040\n",
      "Iteration 237, loss = 1.09476800\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      " Training time: 59.99195098876953s\n",
      "\n",
      " training data\n",
      "\n",
      "[[ 167  764  565 4034]\n",
      " [ 107 3237  415 1824]\n",
      " [ 203  104 3920 1345]\n",
      " [ 149 1359  945 3102]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           2       0.67      0.70      0.69      5572\n",
      "           3       0.30      0.56      0.39      5555\n",
      "           1       0.59      0.58      0.59      5583\n",
      "           0       0.27      0.03      0.05      5530\n",
      "\n",
      "    accuracy                           0.47     22240\n",
      "   macro avg       0.46      0.47      0.43     22240\n",
      "weighted avg       0.46      0.47      0.43     22240\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " test data\n",
      "\n",
      "[[ 20 116  74 438]\n",
      " [ 10 295  83 207]\n",
      " [ 25  38 386 157]\n",
      " [ 14 192 117 300]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       0.27      0.48      0.35       623\n",
      "           1       0.46      0.50      0.48       595\n",
      "           2       0.58      0.64      0.61       606\n",
      "           0       0.29      0.03      0.06       648\n",
      "\n",
      "    accuracy                           0.40      2472\n",
      "   macro avg       0.40      0.41      0.37      2472\n",
      "weighted avg       0.40      0.40      0.37      2472\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(83,41,1), activation='relu', solver='adam', max_iter=500,verbose = True)\n",
    "\n",
    "start = time.time()\n",
    "mlp.fit(X_train,y_train)\n",
    "stop = time.time()\n",
    "\n",
    "predict_train = mlp.predict(X_train)\n",
    "predict_test = mlp.predict(X_test)\n",
    "print(f\"\\n Training time: {stop - start}s\")\n",
    "print(\"\\n training data\\n\")\n",
    "print(confusion_matrix(y_train,predict_train))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_train,predict_train,labels=pd.unique(predict_train)))\n",
    "print(\"\\n---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"\\n test data\\n\")\n",
    "print(confusion_matrix(y_test,predict_test))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,predict_test,labels=pd.unique(predict_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

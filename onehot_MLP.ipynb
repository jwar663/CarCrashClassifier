{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    480452\n",
       "2    480452\n",
       "1    480452\n",
       "0    480452\n",
       "Name: CRASH_SEV_CODE, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "#df = pd.read_csv(\"C:\\\\Users\\Eric\\Desktop\\crash_data_one_hot_encoded_MLP.csv\")\n",
    "df = pd.read_csv(\"crash_data_one_hot_encoded_without_unknowns.csv\")\n",
    "df = sklearn.utils.shuffle(df)\n",
    "df.fillna(0,inplace=True)\n",
    "encode = LabelEncoder()\n",
    "\n",
    "features = ['CRASH_YEAR', 'NUM_LANES', 'SPD_LIM', 'MULTI_VEH_CODE','HOLIDAY_CODE',\n",
    "            'LG_REGION_DESC_CODE', 'JUNCTION_TYPE_CODE', 'DIRN_ROLE1_DESC_CODE','INTSN_MIDBLOCK_CODE',\n",
    "            'FLAT_HILL_CODE','ROAD_CURVATURE_CODE','ROAD_MARKINGS_CODE','ROAD_SURFACE_CODE','ROAD_WET_CODE',\n",
    "            'URBAN_CODE','LIGHT_CODE','STREET_LIGHT_CODE','WEATHER_A_CODE']\n",
    "\n",
    "df_non_injury = df[df.CRASH_SEV_CODE==2]  # 480452 samples\n",
    "df_minor = df[df.CRASH_SEV_CODE==1]       # 150834 samples\n",
    "df_serious = df[df.CRASH_SEV_CODE==3]    # 37347  samples\n",
    "df_fatal = df[df.CRASH_SEV_CODE==0]       # 6178   samples\n",
    "\n",
    "df_minor_upsampled = resample(df_minor, replace=True,n_samples=480452,random_state=40)\n",
    "df_serious_upsampled = resample(df_serious, replace=True,n_samples=480452,random_state=40)\n",
    "df_fatal_upsampled = resample(df_fatal, replace=True,n_samples=480452,random_state=40)\n",
    "\n",
    "df_upsampled = pd.concat([df_non_injury, df_minor_upsampled, df_serious_upsampled, df_fatal_upsampled])\n",
    "\n",
    "df_upsampled.CRASH_SEV_CODE.value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1729627, 92)\n",
      "(192181, 92)\n"
     ]
    }
   ],
   "source": [
    "X = df_upsampled.drop([\"CRASH_SEV_CODE\"], axis=1).values\n",
    "y = df_upsampled[\"CRASH_SEV_CODE\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=40)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.24502974\n",
      "Iteration 2, loss = 1.09342582\n",
      "Iteration 3, loss = 1.05699197\n",
      "Iteration 4, loss = 1.03851845\n",
      "Iteration 5, loss = 1.02600029\n",
      "Iteration 6, loss = 1.01707884\n",
      "Iteration 7, loss = 1.01044924\n",
      "Iteration 8, loss = 1.00525699\n",
      "Iteration 9, loss = 1.00099569\n",
      "Iteration 10, loss = 0.99724328\n",
      "Iteration 11, loss = 0.99403216\n",
      "Iteration 12, loss = 0.99127454\n",
      "Iteration 13, loss = 0.98884170\n",
      "Iteration 14, loss = 0.98661332\n",
      "Iteration 15, loss = 0.98440716\n",
      "Iteration 16, loss = 0.98239979\n",
      "Iteration 17, loss = 0.98072854\n",
      "Iteration 18, loss = 0.97905750\n",
      "Iteration 19, loss = 0.97754919\n",
      "Iteration 20, loss = 0.97608947\n",
      "Iteration 21, loss = 0.97484625\n",
      "Iteration 22, loss = 0.97356071\n",
      "Iteration 23, loss = 0.97217012\n",
      "Iteration 24, loss = 0.97140897\n",
      "Iteration 25, loss = 0.97023478\n",
      "Iteration 26, loss = 0.96915893\n",
      "Iteration 27, loss = 0.96851784\n",
      "Iteration 28, loss = 0.96754928\n",
      "Iteration 29, loss = 0.96680218\n",
      "Iteration 30, loss = 0.96595940\n",
      "Iteration 31, loss = 0.96524767\n",
      "Iteration 32, loss = 0.96449020\n",
      "Iteration 33, loss = 0.96378667\n",
      "Iteration 34, loss = 0.96354228\n",
      "Iteration 35, loss = 0.96280729\n",
      "Iteration 36, loss = 0.96217431\n",
      "Iteration 37, loss = 0.96182172\n",
      "Iteration 38, loss = 0.96092569\n",
      "Iteration 39, loss = 0.96044698\n",
      "Iteration 40, loss = 0.95992215\n",
      "Iteration 41, loss = 0.95943767\n",
      "Iteration 42, loss = 0.95907355\n",
      "Iteration 43, loss = 0.95863114\n",
      "Iteration 44, loss = 0.95807038\n",
      "Iteration 45, loss = 0.95767612\n",
      "Iteration 46, loss = 0.95719320\n",
      "Iteration 47, loss = 0.95700207\n",
      "Iteration 48, loss = 0.95669671\n",
      "Iteration 49, loss = 0.95627763\n",
      "Iteration 50, loss = 0.95603525\n",
      "Iteration 51, loss = 0.95555464\n",
      "Iteration 52, loss = 0.95543631\n",
      "Iteration 53, loss = 0.95489348\n",
      "Iteration 54, loss = 0.95455005\n",
      "Iteration 55, loss = 0.95434455\n",
      "Iteration 56, loss = 0.95389410\n",
      "Iteration 57, loss = 0.95382977\n",
      "Iteration 58, loss = 0.95341497\n",
      "Iteration 59, loss = 0.95339396\n",
      "Iteration 60, loss = 0.95309615\n",
      "Iteration 61, loss = 0.95273000\n",
      "Iteration 62, loss = 0.95251678\n",
      "Iteration 63, loss = 0.95230851\n",
      "Iteration 64, loss = 0.95204794\n",
      "Iteration 65, loss = 0.95182725\n",
      "Iteration 66, loss = 0.95164758\n",
      "Iteration 67, loss = 0.95145944\n",
      "Iteration 68, loss = 0.95107238\n",
      "Iteration 69, loss = 0.95092169\n",
      "Iteration 70, loss = 0.95071115\n",
      "Iteration 71, loss = 0.95047243\n",
      "Iteration 72, loss = 0.95051300\n",
      "Iteration 73, loss = 0.95022585\n",
      "Iteration 74, loss = 0.95010769\n",
      "Iteration 75, loss = 0.95009440\n",
      "Iteration 76, loss = 0.94976366\n",
      "Iteration 77, loss = 0.94941070\n",
      "Iteration 78, loss = 0.94965867\n",
      "Iteration 79, loss = 0.94926873\n",
      "Iteration 80, loss = 0.94915444\n",
      "Iteration 81, loss = 0.94909547\n",
      "Iteration 82, loss = 0.94874565\n",
      "Iteration 83, loss = 0.94872878\n",
      "Iteration 84, loss = 0.94834355\n",
      "Iteration 85, loss = 0.94854023\n",
      "Iteration 86, loss = 0.94813865\n",
      "Iteration 87, loss = 0.94807373\n",
      "Iteration 88, loss = 0.94807583\n",
      "Iteration 89, loss = 0.94774271\n",
      "Iteration 90, loss = 0.94759739\n",
      "Iteration 91, loss = 0.94752575\n",
      "Iteration 92, loss = 0.94754181\n",
      "Iteration 93, loss = 0.94727971\n",
      "Iteration 94, loss = 0.94718673\n",
      "Iteration 95, loss = 0.94696401\n",
      "Iteration 96, loss = 0.94691092\n",
      "Iteration 97, loss = 0.94673896\n",
      "Iteration 98, loss = 0.94668839\n",
      "Iteration 99, loss = 0.94640558\n",
      "Iteration 100, loss = 0.94635693\n",
      "Iteration 101, loss = 0.94619790\n",
      "Iteration 102, loss = 0.94600332\n",
      "Iteration 103, loss = 0.94566226\n",
      "Iteration 104, loss = 0.94558413\n",
      "Iteration 105, loss = 0.94534842\n",
      "Iteration 106, loss = 0.94535030\n",
      "Iteration 107, loss = 0.94527218\n",
      "Iteration 108, loss = 0.94516950\n",
      "Iteration 109, loss = 0.94493713\n",
      "Iteration 110, loss = 0.94492660\n",
      "Iteration 111, loss = 0.94475772\n",
      "Iteration 112, loss = 0.94486166\n",
      "Iteration 113, loss = 0.94465232\n",
      "Iteration 114, loss = 0.94466613\n",
      "Iteration 115, loss = 0.94443632\n",
      "Iteration 116, loss = 0.94447601\n",
      "Iteration 117, loss = 0.94429914\n",
      "Iteration 118, loss = 0.94435111\n",
      "Iteration 119, loss = 0.94430684\n",
      "Iteration 120, loss = 0.94413784\n",
      "Iteration 121, loss = 0.94416644\n",
      "Iteration 122, loss = 0.94396194\n",
      "Iteration 123, loss = 0.94392704\n",
      "Iteration 124, loss = 0.94385606\n",
      "Iteration 125, loss = 0.94366690\n",
      "Iteration 126, loss = 0.94378864\n",
      "Iteration 127, loss = 0.94370670\n",
      "Iteration 128, loss = 0.94354485\n",
      "Iteration 129, loss = 0.94356944\n",
      "Iteration 130, loss = 0.94354213\n",
      "Iteration 131, loss = 0.94362183\n",
      "Iteration 132, loss = 0.94358747\n",
      "Iteration 133, loss = 0.94342010\n",
      "Iteration 134, loss = 0.94334203\n",
      "Iteration 135, loss = 0.94333033\n",
      "Iteration 136, loss = 0.94323037\n",
      "Iteration 137, loss = 0.94306614\n",
      "Iteration 138, loss = 0.94313747\n",
      "Iteration 139, loss = 0.94315864\n",
      "Iteration 140, loss = 0.94318120\n",
      "Iteration 141, loss = 0.94313750\n",
      "Iteration 142, loss = 0.94292597\n",
      "Iteration 143, loss = 0.94287357\n",
      "Iteration 144, loss = 0.94283055\n",
      "Iteration 145, loss = 0.94279356\n",
      "Iteration 146, loss = 0.94277240\n",
      "Iteration 147, loss = 0.94271610\n",
      "Iteration 148, loss = 0.94272615\n",
      "Iteration 149, loss = 0.94251723\n",
      "Iteration 150, loss = 0.94266325\n",
      "Iteration 151, loss = 0.94265891\n",
      "Iteration 152, loss = 0.94247495\n",
      "Iteration 153, loss = 0.94243969\n",
      "Iteration 154, loss = 0.94232452\n",
      "Iteration 155, loss = 0.94233871\n",
      "Iteration 156, loss = 0.94229156\n",
      "Iteration 157, loss = 0.94226964\n",
      "Iteration 158, loss = 0.94239814\n",
      "Iteration 159, loss = 0.94222600\n",
      "Iteration 160, loss = 0.94208344\n",
      "Iteration 161, loss = 0.94208838\n",
      "Iteration 162, loss = 0.94214743\n",
      "Iteration 163, loss = 0.94201891\n",
      "Iteration 164, loss = 0.94214701\n",
      "Iteration 165, loss = 0.94200545\n",
      "Iteration 166, loss = 0.94198062\n",
      "Iteration 167, loss = 0.94186560\n",
      "Iteration 168, loss = 0.94174033\n",
      "Iteration 169, loss = 0.94198039\n",
      "Iteration 170, loss = 0.94175654\n",
      "Iteration 171, loss = 0.94182653\n",
      "Iteration 172, loss = 0.94171113\n",
      "Iteration 173, loss = 0.94176801\n",
      "Iteration 174, loss = 0.94163680\n",
      "Iteration 175, loss = 0.94169070\n",
      "Iteration 176, loss = 0.94160959\n",
      "Iteration 177, loss = 0.94159233\n",
      "Iteration 178, loss = 0.94149099\n",
      "Iteration 179, loss = 0.94156658\n",
      "Iteration 180, loss = 0.94148536\n",
      "Iteration 181, loss = 0.94158283\n",
      "Iteration 182, loss = 0.94153916\n",
      "Iteration 183, loss = 0.94132358\n",
      "Iteration 184, loss = 0.94125837\n",
      "Iteration 185, loss = 0.94134612\n",
      "Iteration 186, loss = 0.94124491\n",
      "Iteration 187, loss = 0.94133096\n",
      "Iteration 188, loss = 0.94126018\n",
      "Iteration 189, loss = 0.94124882\n",
      "Iteration 190, loss = 0.94116466\n",
      "Iteration 191, loss = 0.94111145\n",
      "Iteration 192, loss = 0.94115751\n",
      "Iteration 193, loss = 0.94111006\n",
      "Iteration 194, loss = 0.94104612\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      " Training time: 2821.5127544403076s\n",
      "\n",
      " training data\n",
      "\n",
      "[[414238      0   2425  15737]\n",
      " [ 21992   5780 241207 163488]\n",
      " [ 15824   3812 323776  89220]\n",
      " [ 25066   6708 162248 238106]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.96      0.91    432400\n",
      "           2       0.44      0.75      0.56    432632\n",
      "           1       0.35      0.01      0.03    432467\n",
      "           3       0.47      0.55      0.51    432128\n",
      "\n",
      "    accuracy                           0.57   1729627\n",
      "   macro avg       0.53      0.57      0.50   1729627\n",
      "weighted avg       0.53      0.57      0.50   1729627\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " test data\n",
      "\n",
      "[[46019     0   269  1764]\n",
      " [ 2662   613 26767 17943]\n",
      " [ 1979   405 35643  9793]\n",
      " [ 2950   788 18386 26200]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91     48052\n",
      "           3       0.47      0.54      0.50     48324\n",
      "           2       0.44      0.75      0.55     47820\n",
      "           1       0.34      0.01      0.02     47985\n",
      "\n",
      "    accuracy                           0.56    192181\n",
      "   macro avg       0.53      0.56      0.50    192181\n",
      "weighted avg       0.53      0.56      0.50    192181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(92,45,1), activation='relu', solver='adam', max_iter=500,verbose = True)\n",
    "\n",
    "start = time.time()\n",
    "mlp.fit(X_train,y_train)\n",
    "stop = time.time()\n",
    "\n",
    "predict_train = mlp.predict(X_train)\n",
    "predict_test = mlp.predict(X_test)\n",
    "print(f\"\\n Training time: {stop - start}s\")\n",
    "print(\"\\n training data\\n\")\n",
    "print(confusion_matrix(y_train,predict_train))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_train,predict_train,labels=pd.unique(predict_train)))\n",
    "print(\"\\n---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"\\n test data\\n\")\n",
    "print(confusion_matrix(y_test,predict_test))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,predict_test,labels=pd.unique(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "classification_report(y_test,predict_test,labels=pd.unique(predict_test))\n",
    "print(mlp.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "Norm: 195158.69, NNZs: 18, Bias: 23.000000, T: 1921808, Avg. loss: 686148.332722\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 82456.47, NNZs: 18, Bias: -59.000000, T: 1921808, Avg. loss: 745568.259439\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 116749.24, NNZs: 18, Bias: -29.000000, T: 1921808, Avg. loss: 744053.898060\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 78961.63, NNZs: 18, Bias: 11.000000, T: 1921808, Avg. loss: 672685.401437\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 281231.62, NNZs: 18, Bias: 43.000000, T: 3843616, Avg. loss: 668819.152235\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 128339.92, NNZs: 18, Bias: -118.000000, T: 3843616, Avg. loss: 743692.985429\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 164033.92, NNZs: 18, Bias: -53.000000, T: 3843616, Avg. loss: 736939.681098\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 94703.29, NNZs: 18, Bias: 21.000000, T: 3843616, Avg. loss: 668351.320158\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 281043.54, NNZs: 18, Bias: 62.000000, T: 5765424, Avg. loss: 663922.374089\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 164845.23, NNZs: 18, Bias: -173.000000, T: 5765424, Avg. loss: 742484.267684\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 99212.75, NNZs: 18, Bias: 26.000000, T: 5765424, Avg. loss: 667427.094227\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 180385.45, NNZs: 18, Bias: -71.000000, T: 5765424, Avg. loss: 735944.742280\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 284448.76, NNZs: 18, Bias: 77.000000, T: 7687232, Avg. loss: 666238.806286\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 193309.21, NNZs: 18, Bias: -229.000000, T: 7687232, Avg. loss: 742634.634004\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 178591.61, NNZs: 18, Bias: -90.000000, T: 7687232, Avg. loss: 737281.690980\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 114392.84, NNZs: 18, Bias: 33.000000, T: 7687232, Avg. loss: 667188.410140\n",
      "Total training time: 2.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 267040.42, NNZs: 18, Bias: 92.000000, T: 9609040, Avg. loss: 663701.757230\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 221702.21, NNZs: 18, Bias: -281.000000, T: 9609040, Avg. loss: 740707.935020\n",
      "Total training time: 2.50 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 164507.57, NNZs: 18, Bias: -100.000000, T: 9609040, Avg. loss: 735595.959721\n",
      "Total training time: 2.52 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 128078.52, NNZs: 18, Bias: 41.000000, T: 9609040, Avg. loss: 666085.479523\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 302644.18, NNZs: 18, Bias: 110.000000, T: 11530848, Avg. loss: 665623.670007\n",
      "Total training time: 2.98 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 241944.57, NNZs: 18, Bias: -334.000000, T: 11530848, Avg. loss: 740694.201902\n",
      "Total training time: 3.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 140047.04, NNZs: 18, Bias: 50.000000, T: 11530848, Avg. loss: 665486.938070\n",
      "Total training time: 3.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 179042.76, NNZs: 18, Bias: -114.000000, T: 11530848, Avg. loss: 735330.204818\n",
      "Total training time: 3.03 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 314326.77, NNZs: 18, Bias: 127.000000, T: 13452656, Avg. loss: 665739.303313\n",
      "Total training time: 3.47 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 256793.20, NNZs: 18, Bias: -389.000000, T: 13452656, Avg. loss: 741833.595219\n",
      "Total training time: 3.48 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 152221.91, NNZs: 18, Bias: 58.000000, T: 13452656, Avg. loss: 664778.554352\n",
      "Total training time: 3.50 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 182794.35, NNZs: 18, Bias: -126.000000, T: 13452656, Avg. loss: 735416.718362\n",
      "Total training time: 3.53 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 312677.48, NNZs: 18, Bias: 146.000000, T: 15374464, Avg. loss: 664718.807071\n",
      "Total training time: 3.95 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 275984.99, NNZs: 18, Bias: -442.000000, T: 15374464, Avg. loss: 740790.088620\n",
      "Total training time: 3.97 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 164044.51, NNZs: 18, Bias: 70.000000, T: 15374464, Avg. loss: 664870.584721\n",
      "Total training time: 4.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 193448.74, NNZs: 18, Bias: -141.000000, T: 15374464, Avg. loss: 736198.512778\n",
      "Total training time: 4.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 340929.51, NNZs: 18, Bias: 164.000000, T: 17296272, Avg. loss: 665342.392998\n",
      "Total training time: 4.46 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 291510.20, NNZs: 18, Bias: -492.000000, T: 17296272, Avg. loss: 740458.046660\n",
      "Total training time: 4.47 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 208870.90, NNZs: 18, Bias: -156.000000, T: 17296272, Avg. loss: 735127.921468\n",
      "Total training time: 4.55 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 169751.45, NNZs: 18, Bias: 77.000000, T: 17296272, Avg. loss: 664690.187989\n",
      "Total training time: 4.55 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 315676.18, NNZs: 18, Bias: 178.000000, T: 19218080, Avg. loss: 663358.579583\n",
      "Total training time: 4.94 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 302416.19, NNZs: 18, Bias: -540.000000, T: 19218080, Avg. loss: 741797.961406\n",
      "Total training time: 4.96 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 175227.74, NNZs: 18, Bias: 92.000000, T: 19218080, Avg. loss: 663635.702170\n",
      "Total training time: 5.05 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 202263.14, NNZs: 18, Bias: -166.000000, T: 19218080, Avg. loss: 736005.228902\n",
      "Total training time: 5.06 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 346449.83, NNZs: 18, Bias: 199.000000, T: 21139888, Avg. loss: 665030.129319\n",
      "Total training time: 5.44 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 312863.75, NNZs: 18, Bias: -591.000000, T: 21139888, Avg. loss: 740571.519970\n",
      "Total training time: 5.47 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 180994.86, NNZs: 18, Bias: 101.000000, T: 21139888, Avg. loss: 664583.727511\n",
      "Total training time: 5.58 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 204938.66, NNZs: 18, Bias: -182.000000, T: 21139888, Avg. loss: 736162.327586\n",
      "Total training time: 5.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 317738.09, NNZs: 18, Bias: 213.000000, T: 23061696, Avg. loss: 663940.773949\n",
      "Total training time: 5.93 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 321529.40, NNZs: 18, Bias: -640.000000, T: 23061696, Avg. loss: 740874.798785\n",
      "Total training time: 5.97 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 194419.59, NNZs: 18, Bias: 114.000000, T: 23061696, Avg. loss: 663942.265932\n",
      "Total training time: 6.09 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 212689.47, NNZs: 18, Bias: -199.000000, T: 23061696, Avg. loss: 735096.008495\n",
      "Total training time: 6.10 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 302908.16, NNZs: 18, Bias: 231.000000, T: 24983504, Avg. loss: 663694.587688\n",
      "Total training time: 6.42 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 333625.83, NNZs: 18, Bias: -693.000000, T: 24983504, Avg. loss: 740457.649271\n",
      "Total training time: 6.45 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 192987.60, NNZs: 18, Bias: 125.000000, T: 24983504, Avg. loss: 664458.871953\n",
      "Total training time: 6.59 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 221175.87, NNZs: 18, Bias: -212.000000, T: 24983504, Avg. loss: 734940.608516\n",
      "Total training time: 6.61 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 332653.47, NNZs: 18, Bias: 248.000000, T: 26905312, Avg. loss: 664514.539534\n",
      "Total training time: 6.90 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 339826.06, NNZs: 18, Bias: -743.000000, T: 26905312, Avg. loss: 740865.351133\n",
      "Total training time: 6.94 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 204372.16, NNZs: 18, Bias: 136.000000, T: 26905312, Avg. loss: 663865.339165\n",
      "Total training time: 7.09 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 210676.43, NNZs: 18, Bias: -224.000000, T: 26905312, Avg. loss: 735683.247926\n",
      "Total training time: 7.12 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 333436.39, NNZs: 18, Bias: 263.000000, T: 28827120, Avg. loss: 665603.929677\n",
      "Total training time: 7.40 seconds.\n",
      "Convergence after 15 epochs took 7.40 seconds\n",
      "Norm: 348727.50, NNZs: 18, Bias: -794.000000, T: 28827120, Avg. loss: 740995.184396\n",
      "Total training time: 7.44 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 212273.65, NNZs: 18, Bias: 153.000000, T: 28827120, Avg. loss: 664268.322590\n",
      "Total training time: 7.58 seconds.\n",
      "Convergence after 15 epochs took 7.58 seconds\n",
      "Norm: 240578.95, NNZs: 18, Bias: -241.000000, T: 28827120, Avg. loss: 734738.726650\n",
      "Total training time: 7.59 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 352970.77, NNZs: 18, Bias: -842.000000, T: 30748928, Avg. loss: 741050.593514\n",
      "Total training time: 7.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 251035.54, NNZs: 18, Bias: -258.000000, T: 30748928, Avg. loss: 734902.016352\n",
      "Total training time: 7.99 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 359489.97, NNZs: 18, Bias: -893.000000, T: 32670736, Avg. loss: 741931.772918\n",
      "Total training time: 8.25 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 231948.58, NNZs: 18, Bias: -266.000000, T: 32670736, Avg. loss: 735565.659500\n",
      "Total training time: 8.39 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 370116.30, NNZs: 18, Bias: -944.000000, T: 34592544, Avg. loss: 739542.007891\n",
      "Total training time: 8.62 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 218418.23, NNZs: 18, Bias: -280.000000, T: 34592544, Avg. loss: 735763.668320\n",
      "Total training time: 8.80 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 375234.74, NNZs: 18, Bias: -995.000000, T: 36514352, Avg. loss: 741123.716460\n",
      "Total training time: 9.03 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 221729.52, NNZs: 18, Bias: -294.000000, T: 36514352, Avg. loss: 735453.782756\n",
      "Total training time: 9.23 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 382739.10, NNZs: 18, Bias: -1048.000000, T: 38436160, Avg. loss: 741434.340264\n",
      "Total training time: 9.45 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 223720.15, NNZs: 18, Bias: -305.000000, T: 38436160, Avg. loss: 735461.767411\n",
      "Total training time: 9.63 seconds.\n",
      "Convergence after 20 epochs took 9.63 seconds\n",
      "Norm: 387854.89, NNZs: 18, Bias: -1103.000000, T: 40357968, Avg. loss: 741370.908337\n",
      "Total training time: 9.84 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 392933.55, NNZs: 18, Bias: -1154.000000, T: 42279776, Avg. loss: 740334.383444\n",
      "Total training time: 10.24 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 399304.38, NNZs: 18, Bias: -1205.000000, T: 44201584, Avg. loss: 740429.665863\n",
      "Total training time: 10.64 seconds.\n",
      "Convergence after 23 epochs took 10.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   10.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   10.6s finished\n"
     ]
    }
   ],
   "source": [
    "# only run this cell and below for feature importance\n",
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(tol=1e-3, random_state=40, verbose=True,n_jobs = -1)\n",
    "clf.fit(X,y)\n",
    "\n",
    "coeffs = clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      " min:  -657.505707130214  feature:  ROAD_SURFACE_Unknown \n",
      " max:  37.03248869219892  feature:  MULTI_VEH_Other\n",
      "-------------------\n",
      "-------------------\n",
      " min:  -1315.1133036655588  feature:  DIRN_ROLE1_DESC_0 \n",
      " max:  657.6182417089478  feature:  ROAD_SURFACE_Unknown\n",
      "-------------------\n",
      "-------------------\n",
      " min:  -657.5057071302133  feature:  ROAD_SURFACE_Unknown \n",
      " max:  1315.2258382442915  feature:  DIRN_ROLE1_DESC_0\n",
      "-------------------\n",
      "-------------------\n",
      " min:  -657.5193937681679  feature:  ROAD_SURFACE_Unknown \n",
      " max:  41.12664954461998  feature:  MULTI_VEH_Cyclist(s)+Pedestrian(s) only\n",
      "-------------------\n",
      "\n",
      " training data\n",
      "\n",
      "[[ 94361 101434  51191 185414]\n",
      " [102238 125102 112274  92853]\n",
      " [ 82660 123299 161536  65137]\n",
      " [ 98839 110351  85673 137265]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.22      0.23    432400\n",
      "           2       0.39      0.37      0.38    432632\n",
      "           1       0.27      0.29      0.28    432467\n",
      "           3       0.29      0.32      0.30    432128\n",
      "\n",
      "    accuracy                           0.30   1729627\n",
      "   macro avg       0.30      0.30      0.30   1729627\n",
      "weighted avg       0.30      0.30      0.30   1729627\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " test data\n",
      "\n",
      "[[10569 11318  5664 20501]\n",
      " [11378 13703 12521 10383]\n",
      " [ 9098 13703 17780  7239]\n",
      " [10968 12436  9603 15317]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.22      0.23     48052\n",
      "           2       0.39      0.37      0.38     47820\n",
      "           1       0.27      0.29      0.28     47985\n",
      "           3       0.29      0.32      0.30     48324\n",
      "\n",
      "    accuracy                           0.30    192181\n",
      "   macro avg       0.30      0.30      0.30    192181\n",
      "weighted avg       0.30      0.30      0.30    192181\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df.drop([\"CRASH_SEV_CODE\"], axis=1).columns\n",
    "minV = 0\n",
    "maxV = 0\n",
    "minPlace = 0\n",
    "maxPlace = 0\n",
    "count = 0\n",
    "for i in coeffs:\n",
    "    for j in i:\n",
    "        if count == 0:\n",
    "            minPlace = 0\n",
    "            maxPlace = 0\n",
    "            minV = j\n",
    "            maxV = j\n",
    "        else:\n",
    "            if minV > j:\n",
    "                minV =j\n",
    "                minPlace = count\n",
    "            if maxV < j:\n",
    "                maxV = j\n",
    "                maxPlace = count\n",
    "        count+=1\n",
    "    \n",
    "    count = 0\n",
    "    print(\"-------------------\")\n",
    "    print(\" min: \",minV,\" feature: \",features[minPlace],\"\\n max: \",maxV,\" feature: \",features[maxPlace])\n",
    "    print(\"-------------------\")\n",
    "\n",
    "predict_train2 = clf.predict(X_train)\n",
    "predict_test2 = clf.predict(X_test)\n",
    "\n",
    "print(\"\\n training data\\n\")\n",
    "print(confusion_matrix(y_train,predict_train2))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_train,predict_train2,labels=pd.unique(predict_train)))\n",
    "print(\"\\n---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"\\n test data\\n\")\n",
    "print(confusion_matrix(y_test,predict_test2))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,predict_test2,labels=pd.unique(predict_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    480452\n",
       "2    480452\n",
       "1    480452\n",
       "0    480452\n",
       "Name: CRASH_SEV_CODE, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "#df = pd.read_csv(\"C:\\\\Users\\Eric\\Desktop\\crash_data_one_hot_encoded_MLP.csv\")\n",
    "df = pd.read_csv(\"crash_data_median_clustered.csv\")\n",
    "df2 = pd.read_csv(\"crash_data_only_numeric_values.csv\")\n",
    "\n",
    "#add crash_sev_code to one_hot_encoded\n",
    "df = df.assign(CRASH_SEV_CODE = df2[\"CRASH_SEV_CODE\"])\n",
    "df.drop('CRASH_SEV_F', axis='columns', inplace=True)\n",
    "df.drop('CRASH_SEV_M', axis='columns', inplace=True)\n",
    "df.drop('CRASH_SEV_N', axis='columns', inplace=True)\n",
    "df.drop('CRASH_SEV_S', axis='columns', inplace=True)\n",
    "#for cluster_scaled\n",
    "df.drop('CLUSTER', axis='columns', inplace=True)\n",
    "#for cluster\n",
    "#df.drop('CLUSTER_SCALED', axis='columns', inplace=True)\n",
    "\n",
    "df = sklearn.utils.shuffle(df)\n",
    "df.fillna(0,inplace=True)\n",
    "encode = LabelEncoder()\n",
    "\n",
    "#features = ['CRASH_YEAR', 'NUM_LANES', 'SPD_LIM', 'MULTI_VEH_CODE','HOLIDAY_CODE',\n",
    "#            'LG_REGION_DESC_CODE', 'JUNCTION_TYPE_CODE', 'DIRN_ROLE1_DESC_CODE','INTSN_MIDBLOCK_CODE',\n",
    "#            'FLAT_HILL_CODE','ROAD_CURVATURE_CODE','ROAD_MARKINGS_CODE','ROAD_SURFACE_CODE','ROAD_WET_CODE',\n",
    "#            'URBAN_CODE','LIGHT_CODE','STREET_LIGHT_CODE','WEATHER_A_CODE']\n",
    "\n",
    "features = ['CRASH_YEAR', \n",
    "'NUM_LANES', \n",
    "'SPD_LIM', \n",
    "'TRAFFIC_CTRL_Give Way Sign', \n",
    "'TRAFFIC_CTRL_Nil', \n",
    "'TRAFFIC_CTRL_Points Man', \n",
    "'TRAFFIC_CTRL_School Patrol', \n",
    "'TRAFFIC_CTRL_Stop Sign', \n",
    "'TRAFFIC_CTRL_Traffic Signal', \n",
    "'MULTI_VEH_Cyclist(s)+Pedestrian(s) only', \n",
    "'MULTI_VEH_Cyclists only', \n",
    "'MULTI_VEH_Multi vehicle', \n",
    "'MULTI_VEH_Other', \n",
    "'MULTI_VEH_Others without non-parked veh', \n",
    "'MULTI_VEH_Single vehicle', \n",
    "'MULTI_VEH_Vehicle(s)+Cyclist(s) only', \n",
    "'MULTI_VEH_Vehicle(s)+Pedestrian(s)', \n",
    "'MULTI_VEH_Vehicle(s)+multiple other types', \n",
    "'HOLIDAY_Christmas/New Year', \n",
    "'HOLIDAY_Easter', \n",
    "'HOLIDAY_Labour Weekend', \n",
    "'HOLIDAY_None', \n",
    "'HOLIDAY_Queens Birthday', \n",
    "'LG_REGION_DESC_0', \n",
    "'LG_REGION_DESC_Auckland            ', \n",
    "'LG_REGION_DESC_Bay of Plenty       ', \n",
    "'LG_REGION_DESC_Canterbury          ', \n",
    "'LG_REGION_DESC_Gisborne            ', \n",
    "'LG_REGION_DESC_Hawkes Bay          ', \n",
    "'LG_REGION_DESC_Manawatu/Wanganui   ', \n",
    "'LG_REGION_DESC_Nelson/Marlborough  ', \n",
    "'LG_REGION_DESC_Northland           ', \n",
    "'LG_REGION_DESC_Otago               ', \n",
    "'LG_REGION_DESC_Southland           ', \n",
    "'LG_REGION_DESC_Taranaki            ', \n",
    "'LG_REGION_DESC_Waikato             ', \n",
    "'LG_REGION_DESC_Wellington          ', \n",
    "'LG_REGION_DESC_West Coast          ', \n",
    "'JUNCTION_TYPE_Driveway', \n",
    "'JUNCTION_TYPE_Multi Rd Join', \n",
    "'JUNCTION_TYPE_Roundabout', \n",
    "'JUNCTION_TYPE_T Type Junction',  \n",
    "'JUNCTION_TYPE_X Type Junction', \n",
    "'JUNCTION_TYPE_Y Type Junction', \n",
    "'DIRN_ROLE1_DESC_0', \n",
    "'DIRN_ROLE1_DESC_East', \n",
    "'DIRN_ROLE1_DESC_North', \n",
    "'DIRN_ROLE1_DESC_South',  \n",
    "'DIRN_ROLE1_DESC_West', \n",
    "'INTSN_MIDBLOCK_Intersection', \n",
    "'INTSN_MIDBLOCK_Mid Block', \n",
    "'FLAT_HILL_Flat', \n",
    "'FLAT_HILL_Hill',  \n",
    "'ROAD_CURVATURE_Easy Curve', \n",
    "'ROAD_CURVATURE_Moderate Curve', \n",
    "'ROAD_CURVATURE_Severe Curve', \n",
    "'ROAD_CURVATURE_Straight Road',  \n",
    "'ROAD_MARKINGS_Centre Line', \n",
    "'ROAD_MARKINGS_No Marks', \n",
    "'ROAD_MARKINGS_No Passing Lines', \n",
    "'ROAD_MARKINGS_Painted Island', \n",
    "'ROAD_MARKINGS_Ped Crossing', \n",
    "'ROAD_MARKINGS_Raised Island', \n",
    "'ROAD_SURFACE_Sealed', \n",
    "'ROAD_SURFACE_Unsealed', \n",
    "'ROAD_WET_Dry', \n",
    "'ROAD_WET_Ice/ Snow', \n",
    "'ROAD_WET_Wet', \n",
    "'URBAN_Openroad', \n",
    "'URBAN_Urban', \n",
    "'LIGHT_Bright Sun', \n",
    "'LIGHT_Dark', \n",
    "'LIGHT_Overcast', \n",
    "'LIGHT_Twilight',  \n",
    "'STREET_LIGHT_None', \n",
    "'STREET_LIGHT_Off', \n",
    "'STREET_LIGHT_On',  \n",
    "'WEATHER_A_Fine', \n",
    "'WEATHER_A_Heavy Rain', \n",
    "'WEATHER_A_Light Rain', \n",
    "'WEATHER_A_Mist', \n",
    "'WEATHER_A_Snow',\n",
    "'CLUSTER_SCALED']\n",
    "\n",
    "df_non_injury = df[df.CRASH_SEV_CODE==2]  # 480452 samples\n",
    "df_minor = df[df.CRASH_SEV_CODE==1]       # 150834 samples\n",
    "df_serious = df[df.CRASH_SEV_CODE==3]    # 37347  samples\n",
    "df_fatal = df[df.CRASH_SEV_CODE==0]       # 6178   samples\n",
    "\n",
    "df_minor_upsampled = resample(df_minor, replace=True,n_samples=480452,random_state=40)\n",
    "df_serious_upsampled = resample(df_serious, replace=True,n_samples=480452,random_state=40)\n",
    "df_fatal_upsampled = resample(df_fatal, replace=True,n_samples=480452,random_state=40)\n",
    "\n",
    "df_upsampled = pd.concat([df_non_injury, df_minor_upsampled, df_serious_upsampled, df_fatal_upsampled])\n",
    "\n",
    "df_upsampled.CRASH_SEV_CODE.value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1729627, 83)\n",
      "(192181, 83)\n"
     ]
    }
   ],
   "source": [
    "X = df_upsampled.drop([\"CRASH_SEV_CODE\"], axis=1).values\n",
    "y = df_upsampled[\"CRASH_SEV_CODE\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=40)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.23868929\n",
      "Iteration 2, loss = 1.19889268\n",
      "Iteration 3, loss = 1.18725844\n",
      "Iteration 4, loss = 1.17937626\n",
      "Iteration 5, loss = 1.17375185\n",
      "Iteration 6, loss = 1.16983334\n",
      "Iteration 7, loss = 1.16662963\n",
      "Iteration 8, loss = 1.16384947\n",
      "Iteration 9, loss = 1.16181545\n",
      "Iteration 10, loss = 1.15973700\n",
      "Iteration 11, loss = 1.15797025\n",
      "Iteration 12, loss = 1.15646666\n",
      "Iteration 13, loss = 1.15509414\n",
      "Iteration 14, loss = 1.15393971\n",
      "Iteration 15, loss = 1.15269467\n",
      "Iteration 16, loss = 1.15149351\n",
      "Iteration 17, loss = 1.15050921\n",
      "Iteration 18, loss = 1.14940902\n",
      "Iteration 19, loss = 1.14866623\n",
      "Iteration 20, loss = 1.14758472\n",
      "Iteration 21, loss = 1.14689664\n",
      "Iteration 22, loss = 1.14587164\n",
      "Iteration 23, loss = 1.14521228\n",
      "Iteration 24, loss = 1.14439207\n",
      "Iteration 25, loss = 1.14380818\n",
      "Iteration 26, loss = 1.14318694\n",
      "Iteration 27, loss = 1.14247548\n",
      "Iteration 28, loss = 1.14214262\n",
      "Iteration 29, loss = 1.14150176\n",
      "Iteration 30, loss = 1.14115743\n",
      "Iteration 31, loss = 1.14058261\n",
      "Iteration 32, loss = 1.14016139\n",
      "Iteration 33, loss = 1.13966744\n",
      "Iteration 34, loss = 1.13937076\n",
      "Iteration 35, loss = 1.13888691\n",
      "Iteration 36, loss = 1.13845847\n",
      "Iteration 37, loss = 1.13806352\n",
      "Iteration 38, loss = 1.13776930\n",
      "Iteration 39, loss = 1.13748032\n",
      "Iteration 40, loss = 1.13719933\n",
      "Iteration 41, loss = 1.13668372\n",
      "Iteration 42, loss = 1.13633896\n",
      "Iteration 43, loss = 1.13619342\n",
      "Iteration 44, loss = 1.13579994\n",
      "Iteration 45, loss = 1.13543580\n",
      "Iteration 46, loss = 1.13520133\n",
      "Iteration 47, loss = 1.13486498\n",
      "Iteration 48, loss = 1.13447194\n",
      "Iteration 49, loss = 1.13420610\n",
      "Iteration 50, loss = 1.13398167\n",
      "Iteration 51, loss = 1.13383738\n",
      "Iteration 52, loss = 1.13342596\n",
      "Iteration 53, loss = 1.13321892\n",
      "Iteration 54, loss = 1.13296278\n",
      "Iteration 55, loss = 1.13266085\n",
      "Iteration 56, loss = 1.13242084\n",
      "Iteration 57, loss = 1.13222417\n",
      "Iteration 58, loss = 1.13214845\n",
      "Iteration 59, loss = 1.13191442\n",
      "Iteration 60, loss = 1.13160618\n",
      "Iteration 61, loss = 1.13151296\n",
      "Iteration 62, loss = 1.13133190\n",
      "Iteration 63, loss = 1.13115352\n",
      "Iteration 64, loss = 1.13077891\n",
      "Iteration 65, loss = 1.13067348\n",
      "Iteration 66, loss = 1.13054551\n",
      "Iteration 67, loss = 1.13036768\n",
      "Iteration 68, loss = 1.13005881\n",
      "Iteration 69, loss = 1.12992158\n",
      "Iteration 70, loss = 1.12967625\n",
      "Iteration 71, loss = 1.12968857\n",
      "Iteration 72, loss = 1.12946252\n",
      "Iteration 73, loss = 1.12932255\n",
      "Iteration 74, loss = 1.12914029\n",
      "Iteration 75, loss = 1.12899650\n",
      "Iteration 76, loss = 1.12875818\n",
      "Iteration 77, loss = 1.12872099\n",
      "Iteration 78, loss = 1.12859913\n",
      "Iteration 79, loss = 1.12843008\n",
      "Iteration 80, loss = 1.12825347\n",
      "Iteration 81, loss = 1.12812035\n",
      "Iteration 82, loss = 1.12798626\n",
      "Iteration 83, loss = 1.12792849\n",
      "Iteration 84, loss = 1.12784104\n",
      "Iteration 85, loss = 1.12772232\n",
      "Iteration 86, loss = 1.12770043\n",
      "Iteration 87, loss = 1.12746448\n",
      "Iteration 88, loss = 1.12728027\n",
      "Iteration 89, loss = 1.12707347\n",
      "Iteration 90, loss = 1.12706169\n",
      "Iteration 91, loss = 1.12681050\n",
      "Iteration 92, loss = 1.12679868\n",
      "Iteration 93, loss = 1.12670456\n",
      "Iteration 94, loss = 1.12671126\n",
      "Iteration 95, loss = 1.12641600\n",
      "Iteration 96, loss = 1.12645130\n",
      "Iteration 97, loss = 1.12636270\n",
      "Iteration 98, loss = 1.12613289\n",
      "Iteration 99, loss = 1.12598036\n",
      "Iteration 100, loss = 1.12595217\n",
      "Iteration 101, loss = 1.12586882\n",
      "Iteration 102, loss = 1.12584730\n",
      "Iteration 103, loss = 1.12563559\n",
      "Iteration 104, loss = 1.12559002\n",
      "Iteration 105, loss = 1.12553907\n",
      "Iteration 106, loss = 1.12541743\n",
      "Iteration 107, loss = 1.12533976\n",
      "Iteration 108, loss = 1.12540552\n",
      "Iteration 109, loss = 1.12503952\n",
      "Iteration 110, loss = 1.12519459\n",
      "Iteration 111, loss = 1.12504678\n",
      "Iteration 112, loss = 1.12488181\n",
      "Iteration 113, loss = 1.12491313\n",
      "Iteration 114, loss = 1.12481010\n",
      "Iteration 115, loss = 1.12465467\n",
      "Iteration 116, loss = 1.12475376\n",
      "Iteration 117, loss = 1.12462824\n",
      "Iteration 118, loss = 1.12458934\n",
      "Iteration 119, loss = 1.12454765\n",
      "Iteration 120, loss = 1.12428887\n",
      "Iteration 121, loss = 1.12433090\n",
      "Iteration 122, loss = 1.12418812\n",
      "Iteration 123, loss = 1.12413591\n",
      "Iteration 124, loss = 1.12401634\n",
      "Iteration 125, loss = 1.12401592\n",
      "Iteration 126, loss = 1.12396179\n",
      "Iteration 127, loss = 1.12390456\n",
      "Iteration 128, loss = 1.12387701\n",
      "Iteration 129, loss = 1.12367785\n",
      "Iteration 130, loss = 1.12371790\n",
      "Iteration 131, loss = 1.12354524\n",
      "Iteration 132, loss = 1.12349039\n",
      "Iteration 133, loss = 1.12347703\n",
      "Iteration 134, loss = 1.12346513\n",
      "Iteration 135, loss = 1.12344464\n",
      "Iteration 136, loss = 1.12333221\n",
      "Iteration 137, loss = 1.12332128\n",
      "Iteration 138, loss = 1.12332001\n",
      "Iteration 139, loss = 1.12320519\n",
      "Iteration 140, loss = 1.12318764\n",
      "Iteration 141, loss = 1.12312427\n",
      "Iteration 142, loss = 1.12306791\n",
      "Iteration 143, loss = 1.12308899\n",
      "Iteration 144, loss = 1.12295461\n",
      "Iteration 145, loss = 1.12289852\n",
      "Iteration 146, loss = 1.12278372\n",
      "Iteration 147, loss = 1.12270622\n",
      "Iteration 148, loss = 1.12273263\n",
      "Iteration 149, loss = 1.12287803\n",
      "Iteration 150, loss = 1.12265521\n",
      "Iteration 151, loss = 1.12253939\n",
      "Iteration 152, loss = 1.12254847\n",
      "Iteration 153, loss = 1.12253670\n",
      "Iteration 154, loss = 1.12241042\n",
      "Iteration 155, loss = 1.12243809\n",
      "Iteration 156, loss = 1.12245093\n",
      "Iteration 157, loss = 1.12221597\n",
      "Iteration 158, loss = 1.12223483\n",
      "Iteration 159, loss = 1.12215623\n",
      "Iteration 160, loss = 1.12201852\n",
      "Iteration 161, loss = 1.12224157\n",
      "Iteration 162, loss = 1.12197386\n",
      "Iteration 163, loss = 1.12182173\n",
      "Iteration 164, loss = 1.12186592\n",
      "Iteration 165, loss = 1.12196114\n",
      "Iteration 166, loss = 1.12177829\n",
      "Iteration 167, loss = 1.12176974\n",
      "Iteration 168, loss = 1.12167407\n",
      "Iteration 169, loss = 1.12153178\n",
      "Iteration 170, loss = 1.12147085\n",
      "Iteration 171, loss = 1.12080189\n",
      "Iteration 172, loss = 1.12056835\n",
      "Iteration 173, loss = 1.11909128\n",
      "Iteration 174, loss = 1.11862232\n",
      "Iteration 175, loss = 1.11825181\n",
      "Iteration 176, loss = 1.11787194\n",
      "Iteration 177, loss = 1.11748777\n",
      "Iteration 178, loss = 1.11736710\n",
      "Iteration 179, loss = 1.11737932\n",
      "Iteration 180, loss = 1.11715328\n",
      "Iteration 181, loss = 1.11722850\n",
      "Iteration 182, loss = 1.11702008\n",
      "Iteration 183, loss = 1.11708048\n",
      "Iteration 184, loss = 1.11706372\n",
      "Iteration 185, loss = 1.11640073\n",
      "Iteration 186, loss = 1.11621534\n",
      "Iteration 187, loss = 1.11608731\n",
      "Iteration 188, loss = 1.11614268\n",
      "Iteration 189, loss = 1.11577806\n",
      "Iteration 190, loss = 1.11592072\n",
      "Iteration 191, loss = 1.11592989\n",
      "Iteration 192, loss = 1.11579297\n",
      "Iteration 193, loss = 1.11585034\n",
      "Iteration 194, loss = 1.11571447\n",
      "Iteration 195, loss = 1.11583596\n",
      "Iteration 196, loss = 1.11581062\n",
      "Iteration 197, loss = 1.11542844\n",
      "Iteration 198, loss = 1.11559560\n",
      "Iteration 199, loss = 1.11565458\n",
      "Iteration 200, loss = 1.11544387\n",
      "Iteration 201, loss = 1.11531288\n",
      "Iteration 202, loss = 1.11524278\n",
      "Iteration 203, loss = 1.11509778\n",
      "Iteration 204, loss = 1.11528023\n",
      "Iteration 205, loss = 1.11497460\n",
      "Iteration 206, loss = 1.11494561\n",
      "Iteration 207, loss = 1.11503300\n",
      "Iteration 208, loss = 1.11503937\n",
      "Iteration 209, loss = 1.11501722\n",
      "Iteration 210, loss = 1.11495303\n",
      "Iteration 211, loss = 1.11479377\n",
      "Iteration 212, loss = 1.11483097\n",
      "Iteration 213, loss = 1.11486037\n",
      "Iteration 214, loss = 1.11483659\n",
      "Iteration 215, loss = 1.11466365\n",
      "Iteration 216, loss = 1.11470546\n",
      "Iteration 217, loss = 1.11476947\n",
      "Iteration 218, loss = 1.11459564\n",
      "Iteration 219, loss = 1.11455627\n",
      "Iteration 220, loss = 1.11468574\n",
      "Iteration 221, loss = 1.11451587\n",
      "Iteration 222, loss = 1.11447953\n",
      "Iteration 223, loss = 1.11445480\n",
      "Iteration 224, loss = 1.11429681\n",
      "Iteration 225, loss = 1.11452412\n",
      "Iteration 226, loss = 1.11446719\n",
      "Iteration 227, loss = 1.11430304\n",
      "Iteration 228, loss = 1.11438371\n",
      "Iteration 229, loss = 1.11416193\n",
      "Iteration 230, loss = 1.11425456\n",
      "Iteration 231, loss = 1.11418639\n",
      "Iteration 232, loss = 1.11423574\n",
      "Iteration 233, loss = 1.11408862\n",
      "Iteration 234, loss = 1.11421625\n",
      "Iteration 235, loss = 1.11405739\n",
      "Iteration 236, loss = 1.11404167\n",
      "Iteration 237, loss = 1.11405956\n",
      "Iteration 238, loss = 1.11420113\n",
      "Iteration 239, loss = 1.11391879\n",
      "Iteration 240, loss = 1.11394013\n",
      "Iteration 241, loss = 1.11395415\n",
      "Iteration 242, loss = 1.11396735\n",
      "Iteration 243, loss = 1.11390164\n",
      "Iteration 244, loss = 1.11400974\n",
      "Iteration 245, loss = 1.11382394\n",
      "Iteration 246, loss = 1.11379279\n",
      "Iteration 247, loss = 1.11375380\n",
      "Iteration 248, loss = 1.11377220\n",
      "Iteration 249, loss = 1.11361022\n",
      "Iteration 250, loss = 1.11356514\n",
      "Iteration 251, loss = 1.11370119\n",
      "Iteration 252, loss = 1.11336778\n",
      "Iteration 253, loss = 1.11351909\n",
      "Iteration 254, loss = 1.11353002\n",
      "Iteration 255, loss = 1.11350497\n",
      "Iteration 256, loss = 1.11336540\n",
      "Iteration 257, loss = 1.11369096\n",
      "Iteration 258, loss = 1.11339990\n",
      "Iteration 259, loss = 1.11345833\n",
      "Iteration 260, loss = 1.11335566\n",
      "Iteration 261, loss = 1.11334947\n",
      "Iteration 262, loss = 1.11338803\n",
      "Iteration 263, loss = 1.11334577\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      " Training time: 1342.373289823532s\n",
      "\n",
      " training data\n",
      "\n",
      "[[387157      0  36967   8276]\n",
      " [201636 147779  66828  16224]\n",
      " [120841      1 311768     22]\n",
      " [281473  35500  89437  25718]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.90      0.54    432400\n",
      "           1       0.81      0.34      0.48    432467\n",
      "           2       0.62      0.72      0.67    432632\n",
      "           3       0.51      0.06      0.11    432128\n",
      "\n",
      "    accuracy                           0.50   1729627\n",
      "   macro avg       0.58      0.50      0.45   1729627\n",
      "weighted avg       0.58      0.50      0.45   1729627\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " test data\n",
      "\n",
      "[[42884     0  4225   943]\n",
      " [22192 16617  7363  1813]\n",
      " [13614     1 34195    10]\n",
      " [31328  3994 10173  2829]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.39      0.89      0.54     48052\n",
      "           3       0.51      0.06      0.10     48324\n",
      "           1       0.81      0.35      0.48     47985\n",
      "           2       0.61      0.72      0.66     47820\n",
      "\n",
      "    accuracy                           0.50    192181\n",
      "   macro avg       0.58      0.50      0.45    192181\n",
      "weighted avg       0.58      0.50      0.45    192181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(83,41,1), activation='relu', solver='adam', max_iter=500,verbose = True)\n",
    "\n",
    "start = time.time()\n",
    "mlp.fit(X_train,y_train)\n",
    "stop = time.time()\n",
    "\n",
    "predict_train = mlp.predict(X_train)\n",
    "predict_test = mlp.predict(X_test)\n",
    "print(f\"\\n Training time: {stop - start}s\")\n",
    "print(\"\\n training data\\n\")\n",
    "print(confusion_matrix(y_train,predict_train))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_train,predict_train,labels=pd.unique(predict_train)))\n",
    "print(\"\\n---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"\\n test data\\n\")\n",
    "print(confusion_matrix(y_test,predict_test))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,predict_test,labels=pd.unique(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "classification_report(y_test,predict_test,labels=pd.unique(predict_test))\n",
    "print(mlp.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "Norm: 195158.69, NNZs: 18, Bias: 23.000000, T: 1921808, Avg. loss: 686148.332722\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 82456.47, NNZs: 18, Bias: -59.000000, T: 1921808, Avg. loss: 745568.259439\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 116749.24, NNZs: 18, Bias: -29.000000, T: 1921808, Avg. loss: 744053.898060\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 78961.63, NNZs: 18, Bias: 11.000000, T: 1921808, Avg. loss: 672685.401437\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 281231.62, NNZs: 18, Bias: 43.000000, T: 3843616, Avg. loss: 668819.152235\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 128339.92, NNZs: 18, Bias: -118.000000, T: 3843616, Avg. loss: 743692.985429\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 164033.92, NNZs: 18, Bias: -53.000000, T: 3843616, Avg. loss: 736939.681098\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 94703.29, NNZs: 18, Bias: 21.000000, T: 3843616, Avg. loss: 668351.320158\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 281043.54, NNZs: 18, Bias: 62.000000, T: 5765424, Avg. loss: 663922.374089\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 164845.23, NNZs: 18, Bias: -173.000000, T: 5765424, Avg. loss: 742484.267684\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 99212.75, NNZs: 18, Bias: 26.000000, T: 5765424, Avg. loss: 667427.094227\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 180385.45, NNZs: 18, Bias: -71.000000, T: 5765424, Avg. loss: 735944.742280\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 284448.76, NNZs: 18, Bias: 77.000000, T: 7687232, Avg. loss: 666238.806286\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 193309.21, NNZs: 18, Bias: -229.000000, T: 7687232, Avg. loss: 742634.634004\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 178591.61, NNZs: 18, Bias: -90.000000, T: 7687232, Avg. loss: 737281.690980\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 114392.84, NNZs: 18, Bias: 33.000000, T: 7687232, Avg. loss: 667188.410140\n",
      "Total training time: 2.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 267040.42, NNZs: 18, Bias: 92.000000, T: 9609040, Avg. loss: 663701.757230\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 221702.21, NNZs: 18, Bias: -281.000000, T: 9609040, Avg. loss: 740707.935020\n",
      "Total training time: 2.50 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 164507.57, NNZs: 18, Bias: -100.000000, T: 9609040, Avg. loss: 735595.959721\n",
      "Total training time: 2.52 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 128078.52, NNZs: 18, Bias: 41.000000, T: 9609040, Avg. loss: 666085.479523\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 302644.18, NNZs: 18, Bias: 110.000000, T: 11530848, Avg. loss: 665623.670007\n",
      "Total training time: 2.98 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 241944.57, NNZs: 18, Bias: -334.000000, T: 11530848, Avg. loss: 740694.201902\n",
      "Total training time: 3.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 140047.04, NNZs: 18, Bias: 50.000000, T: 11530848, Avg. loss: 665486.938070\n",
      "Total training time: 3.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 179042.76, NNZs: 18, Bias: -114.000000, T: 11530848, Avg. loss: 735330.204818\n",
      "Total training time: 3.03 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 314326.77, NNZs: 18, Bias: 127.000000, T: 13452656, Avg. loss: 665739.303313\n",
      "Total training time: 3.47 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 256793.20, NNZs: 18, Bias: -389.000000, T: 13452656, Avg. loss: 741833.595219\n",
      "Total training time: 3.48 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 152221.91, NNZs: 18, Bias: 58.000000, T: 13452656, Avg. loss: 664778.554352\n",
      "Total training time: 3.50 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 182794.35, NNZs: 18, Bias: -126.000000, T: 13452656, Avg. loss: 735416.718362\n",
      "Total training time: 3.53 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 312677.48, NNZs: 18, Bias: 146.000000, T: 15374464, Avg. loss: 664718.807071\n",
      "Total training time: 3.95 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 275984.99, NNZs: 18, Bias: -442.000000, T: 15374464, Avg. loss: 740790.088620\n",
      "Total training time: 3.97 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 164044.51, NNZs: 18, Bias: 70.000000, T: 15374464, Avg. loss: 664870.584721\n",
      "Total training time: 4.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 193448.74, NNZs: 18, Bias: -141.000000, T: 15374464, Avg. loss: 736198.512778\n",
      "Total training time: 4.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 340929.51, NNZs: 18, Bias: 164.000000, T: 17296272, Avg. loss: 665342.392998\n",
      "Total training time: 4.46 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 291510.20, NNZs: 18, Bias: -492.000000, T: 17296272, Avg. loss: 740458.046660\n",
      "Total training time: 4.47 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 208870.90, NNZs: 18, Bias: -156.000000, T: 17296272, Avg. loss: 735127.921468\n",
      "Total training time: 4.55 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 169751.45, NNZs: 18, Bias: 77.000000, T: 17296272, Avg. loss: 664690.187989\n",
      "Total training time: 4.55 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 315676.18, NNZs: 18, Bias: 178.000000, T: 19218080, Avg. loss: 663358.579583\n",
      "Total training time: 4.94 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 302416.19, NNZs: 18, Bias: -540.000000, T: 19218080, Avg. loss: 741797.961406\n",
      "Total training time: 4.96 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 175227.74, NNZs: 18, Bias: 92.000000, T: 19218080, Avg. loss: 663635.702170\n",
      "Total training time: 5.05 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 202263.14, NNZs: 18, Bias: -166.000000, T: 19218080, Avg. loss: 736005.228902\n",
      "Total training time: 5.06 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 346449.83, NNZs: 18, Bias: 199.000000, T: 21139888, Avg. loss: 665030.129319\n",
      "Total training time: 5.44 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 312863.75, NNZs: 18, Bias: -591.000000, T: 21139888, Avg. loss: 740571.519970\n",
      "Total training time: 5.47 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 180994.86, NNZs: 18, Bias: 101.000000, T: 21139888, Avg. loss: 664583.727511\n",
      "Total training time: 5.58 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 204938.66, NNZs: 18, Bias: -182.000000, T: 21139888, Avg. loss: 736162.327586\n",
      "Total training time: 5.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 317738.09, NNZs: 18, Bias: 213.000000, T: 23061696, Avg. loss: 663940.773949\n",
      "Total training time: 5.93 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 321529.40, NNZs: 18, Bias: -640.000000, T: 23061696, Avg. loss: 740874.798785\n",
      "Total training time: 5.97 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 194419.59, NNZs: 18, Bias: 114.000000, T: 23061696, Avg. loss: 663942.265932\n",
      "Total training time: 6.09 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 212689.47, NNZs: 18, Bias: -199.000000, T: 23061696, Avg. loss: 735096.008495\n",
      "Total training time: 6.10 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 302908.16, NNZs: 18, Bias: 231.000000, T: 24983504, Avg. loss: 663694.587688\n",
      "Total training time: 6.42 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 333625.83, NNZs: 18, Bias: -693.000000, T: 24983504, Avg. loss: 740457.649271\n",
      "Total training time: 6.45 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 192987.60, NNZs: 18, Bias: 125.000000, T: 24983504, Avg. loss: 664458.871953\n",
      "Total training time: 6.59 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 221175.87, NNZs: 18, Bias: -212.000000, T: 24983504, Avg. loss: 734940.608516\n",
      "Total training time: 6.61 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 332653.47, NNZs: 18, Bias: 248.000000, T: 26905312, Avg. loss: 664514.539534\n",
      "Total training time: 6.90 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 339826.06, NNZs: 18, Bias: -743.000000, T: 26905312, Avg. loss: 740865.351133\n",
      "Total training time: 6.94 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 204372.16, NNZs: 18, Bias: 136.000000, T: 26905312, Avg. loss: 663865.339165\n",
      "Total training time: 7.09 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 210676.43, NNZs: 18, Bias: -224.000000, T: 26905312, Avg. loss: 735683.247926\n",
      "Total training time: 7.12 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 333436.39, NNZs: 18, Bias: 263.000000, T: 28827120, Avg. loss: 665603.929677\n",
      "Total training time: 7.40 seconds.\n",
      "Convergence after 15 epochs took 7.40 seconds\n",
      "Norm: 348727.50, NNZs: 18, Bias: -794.000000, T: 28827120, Avg. loss: 740995.184396\n",
      "Total training time: 7.44 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 212273.65, NNZs: 18, Bias: 153.000000, T: 28827120, Avg. loss: 664268.322590\n",
      "Total training time: 7.58 seconds.\n",
      "Convergence after 15 epochs took 7.58 seconds\n",
      "Norm: 240578.95, NNZs: 18, Bias: -241.000000, T: 28827120, Avg. loss: 734738.726650\n",
      "Total training time: 7.59 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 352970.77, NNZs: 18, Bias: -842.000000, T: 30748928, Avg. loss: 741050.593514\n",
      "Total training time: 7.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 251035.54, NNZs: 18, Bias: -258.000000, T: 30748928, Avg. loss: 734902.016352\n",
      "Total training time: 7.99 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 359489.97, NNZs: 18, Bias: -893.000000, T: 32670736, Avg. loss: 741931.772918\n",
      "Total training time: 8.25 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 231948.58, NNZs: 18, Bias: -266.000000, T: 32670736, Avg. loss: 735565.659500\n",
      "Total training time: 8.39 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 370116.30, NNZs: 18, Bias: -944.000000, T: 34592544, Avg. loss: 739542.007891\n",
      "Total training time: 8.62 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 218418.23, NNZs: 18, Bias: -280.000000, T: 34592544, Avg. loss: 735763.668320\n",
      "Total training time: 8.80 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 375234.74, NNZs: 18, Bias: -995.000000, T: 36514352, Avg. loss: 741123.716460\n",
      "Total training time: 9.03 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 221729.52, NNZs: 18, Bias: -294.000000, T: 36514352, Avg. loss: 735453.782756\n",
      "Total training time: 9.23 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 382739.10, NNZs: 18, Bias: -1048.000000, T: 38436160, Avg. loss: 741434.340264\n",
      "Total training time: 9.45 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 223720.15, NNZs: 18, Bias: -305.000000, T: 38436160, Avg. loss: 735461.767411\n",
      "Total training time: 9.63 seconds.\n",
      "Convergence after 20 epochs took 9.63 seconds\n",
      "Norm: 387854.89, NNZs: 18, Bias: -1103.000000, T: 40357968, Avg. loss: 741370.908337\n",
      "Total training time: 9.84 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 392933.55, NNZs: 18, Bias: -1154.000000, T: 42279776, Avg. loss: 740334.383444\n",
      "Total training time: 10.24 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 399304.38, NNZs: 18, Bias: -1205.000000, T: 44201584, Avg. loss: 740429.665863\n",
      "Total training time: 10.64 seconds.\n",
      "Convergence after 23 epochs took 10.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   10.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   10.6s finished\n"
     ]
    }
   ],
   "source": [
    "# only run this cell and below for feature importance\n",
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(tol=1e-3, random_state=40, verbose=True,n_jobs = -1)\n",
    "clf.fit(X,y)\n",
    "\n",
    "coeffs = clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      " min:  -657.505707130214  feature:  ROAD_SURFACE_Unknown \n",
      " max:  37.03248869219892  feature:  MULTI_VEH_Other\n",
      "-------------------\n",
      "-------------------\n",
      " min:  -1315.1133036655588  feature:  DIRN_ROLE1_DESC_0 \n",
      " max:  657.6182417089478  feature:  ROAD_SURFACE_Unknown\n",
      "-------------------\n",
      "-------------------\n",
      " min:  -657.5057071302133  feature:  ROAD_SURFACE_Unknown \n",
      " max:  1315.2258382442915  feature:  DIRN_ROLE1_DESC_0\n",
      "-------------------\n",
      "-------------------\n",
      " min:  -657.5193937681679  feature:  ROAD_SURFACE_Unknown \n",
      " max:  41.12664954461998  feature:  MULTI_VEH_Cyclist(s)+Pedestrian(s) only\n",
      "-------------------\n",
      "\n",
      " training data\n",
      "\n",
      "[[ 94361 101434  51191 185414]\n",
      " [102238 125102 112274  92853]\n",
      " [ 82660 123299 161536  65137]\n",
      " [ 98839 110351  85673 137265]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.22      0.23    432400\n",
      "           2       0.39      0.37      0.38    432632\n",
      "           1       0.27      0.29      0.28    432467\n",
      "           3       0.29      0.32      0.30    432128\n",
      "\n",
      "    accuracy                           0.30   1729627\n",
      "   macro avg       0.30      0.30      0.30   1729627\n",
      "weighted avg       0.30      0.30      0.30   1729627\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " test data\n",
      "\n",
      "[[10569 11318  5664 20501]\n",
      " [11378 13703 12521 10383]\n",
      " [ 9098 13703 17780  7239]\n",
      " [10968 12436  9603 15317]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.22      0.23     48052\n",
      "           2       0.39      0.37      0.38     47820\n",
      "           1       0.27      0.29      0.28     47985\n",
      "           3       0.29      0.32      0.30     48324\n",
      "\n",
      "    accuracy                           0.30    192181\n",
      "   macro avg       0.30      0.30      0.30    192181\n",
      "weighted avg       0.30      0.30      0.30    192181\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df.drop([\"CRASH_SEV_CODE\"], axis=1).columns\n",
    "minV = 0\n",
    "maxV = 0\n",
    "minPlace = 0\n",
    "maxPlace = 0\n",
    "count = 0\n",
    "for i in coeffs:\n",
    "    for j in i:\n",
    "        if count == 0:\n",
    "            minPlace = 0\n",
    "            maxPlace = 0\n",
    "            minV = j\n",
    "            maxV = j\n",
    "        else:\n",
    "            if minV > j:\n",
    "                minV =j\n",
    "                minPlace = count\n",
    "            if maxV < j:\n",
    "                maxV = j\n",
    "                maxPlace = count\n",
    "        count+=1\n",
    "    \n",
    "    count = 0\n",
    "    print(\"-------------------\")\n",
    "    print(\" min: \",minV,\" feature: \",features[minPlace],\"\\n max: \",maxV,\" feature: \",features[maxPlace])\n",
    "    print(\"-------------------\")\n",
    "\n",
    "predict_train2 = clf.predict(X_train)\n",
    "predict_test2 = clf.predict(X_test)\n",
    "\n",
    "print(\"\\n training data\\n\")\n",
    "print(confusion_matrix(y_train,predict_train2))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_train,predict_train2,labels=pd.unique(predict_train)))\n",
    "print(\"\\n---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"\\n test data\\n\")\n",
    "print(confusion_matrix(y_test,predict_test2))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,predict_test2,labels=pd.unique(predict_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

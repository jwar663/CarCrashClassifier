{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    480452\n",
       "2    480452\n",
       "1    480452\n",
       "0    480452\n",
       "Name: CRASH_SEV_CODE, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.utils import resample\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "\n",
    "#df = pd.read_csv(\"C:\\\\Users\\Eric\\Desktop\\crash_data_one_hot_encoded_MLP.csv\")\n",
    "df = pd.read_csv(\"crash_data_no_unknowns_clustered.csv\")\n",
    "df2 = pd.read_csv(\"crash_data_only_numeric_values.csv\")\n",
    "\n",
    "#add crash_sev_code to one_hot_encoded\n",
    "df = df.assign(CRASH_SEV_CODE = df2[\"CRASH_SEV_CODE\"])\n",
    "df.drop('CRASH_SEV_F', axis='columns', inplace=True)\n",
    "df.drop('CRASH_SEV_M', axis='columns', inplace=True)\n",
    "df.drop('CRASH_SEV_N', axis='columns', inplace=True)\n",
    "df.drop('CRASH_SEV_S', axis='columns', inplace=True)\n",
    "#for cluster_scaled\n",
    "df.drop('CLUSTER', axis='columns', inplace=True)\n",
    "#for cluster\n",
    "#df.drop('CLUSTER_SCALED', axis='columns', inplace=True)\n",
    "\n",
    "df = sklearn.utils.shuffle(df)\n",
    "df.fillna(0,inplace=True)\n",
    "encode = LabelEncoder()\n",
    "\n",
    "#features = ['CRASH_YEAR', 'NUM_LANES', 'SPD_LIM', 'MULTI_VEH_CODE','HOLIDAY_CODE',\n",
    "#            'LG_REGION_DESC_CODE', 'JUNCTION_TYPE_CODE', 'DIRN_ROLE1_DESC_CODE','INTSN_MIDBLOCK_CODE',\n",
    "#            'FLAT_HILL_CODE','ROAD_CURVATURE_CODE','ROAD_MARKINGS_CODE','ROAD_SURFACE_CODE','ROAD_WET_CODE',\n",
    "#            'URBAN_CODE','LIGHT_CODE','STREET_LIGHT_CODE','WEATHER_A_CODE']\n",
    "\n",
    "features = ['CRASH_YEAR', \n",
    "'NUM_LANES', \n",
    "'SPD_LIM', \n",
    "'TRAFFIC_CTRL_Give Way Sign', \n",
    "'TRAFFIC_CTRL_Nil', \n",
    "'TRAFFIC_CTRL_Points Man', \n",
    "'TRAFFIC_CTRL_School Patrol', \n",
    "'TRAFFIC_CTRL_Stop Sign', \n",
    "'TRAFFIC_CTRL_Traffic Signal', \n",
    "'MULTI_VEH_Cyclist(s)+Pedestrian(s) only', \n",
    "'MULTI_VEH_Cyclists only', \n",
    "'MULTI_VEH_Multi vehicle', \n",
    "'MULTI_VEH_Other', \n",
    "'MULTI_VEH_Others without non-parked veh', \n",
    "'MULTI_VEH_Single vehicle', \n",
    "'MULTI_VEH_Vehicle(s)+Cyclist(s) only', \n",
    "'MULTI_VEH_Vehicle(s)+Pedestrian(s)', \n",
    "'MULTI_VEH_Vehicle(s)+multiple other types', \n",
    "'HOLIDAY_Christmas/New Year', \n",
    "'HOLIDAY_Easter', \n",
    "'HOLIDAY_Labour Weekend', \n",
    "'HOLIDAY_None', \n",
    "'HOLIDAY_Queens Birthday', \n",
    "'LG_REGION_DESC_0', \n",
    "'LG_REGION_DESC_Auckland            ', \n",
    "'LG_REGION_DESC_Bay of Plenty       ', \n",
    "'LG_REGION_DESC_Canterbury          ', \n",
    "'LG_REGION_DESC_Gisborne            ', \n",
    "'LG_REGION_DESC_Hawkes Bay          ', \n",
    "'LG_REGION_DESC_Manawatu/Wanganui   ', \n",
    "'LG_REGION_DESC_Nelson/Marlborough  ', \n",
    "'LG_REGION_DESC_Northland           ', \n",
    "'LG_REGION_DESC_Otago               ', \n",
    "'LG_REGION_DESC_Southland           ', \n",
    "'LG_REGION_DESC_Taranaki            ', \n",
    "'LG_REGION_DESC_Waikato             ', \n",
    "'LG_REGION_DESC_Wellington          ', \n",
    "'LG_REGION_DESC_West Coast          ', \n",
    "'JUNCTION_TYPE_Driveway', \n",
    "'JUNCTION_TYPE_Multi Rd Join', \n",
    "'JUNCTION_TYPE_Roundabout', \n",
    "'JUNCTION_TYPE_T Type Junction',  \n",
    "'JUNCTION_TYPE_X Type Junction', \n",
    "'JUNCTION_TYPE_Y Type Junction', \n",
    "'DIRN_ROLE1_DESC_0', \n",
    "'DIRN_ROLE1_DESC_East', \n",
    "'DIRN_ROLE1_DESC_North', \n",
    "'DIRN_ROLE1_DESC_South',  \n",
    "'DIRN_ROLE1_DESC_West', \n",
    "'INTSN_MIDBLOCK_Intersection', \n",
    "'INTSN_MIDBLOCK_Mid Block', \n",
    "'FLAT_HILL_Flat', \n",
    "'FLAT_HILL_Hill',  \n",
    "'ROAD_CURVATURE_Easy Curve', \n",
    "'ROAD_CURVATURE_Moderate Curve', \n",
    "'ROAD_CURVATURE_Severe Curve', \n",
    "'ROAD_CURVATURE_Straight Road',  \n",
    "'ROAD_MARKINGS_Centre Line', \n",
    "'ROAD_MARKINGS_No Marks', \n",
    "'ROAD_MARKINGS_No Passing Lines', \n",
    "'ROAD_MARKINGS_Painted Island', \n",
    "'ROAD_MARKINGS_Ped Crossing', \n",
    "'ROAD_MARKINGS_Raised Island', \n",
    "'ROAD_SURFACE_Sealed', \n",
    "'ROAD_SURFACE_Unsealed', \n",
    "'ROAD_WET_Dry', \n",
    "'ROAD_WET_Ice/ Snow', \n",
    "'ROAD_WET_Wet', \n",
    "'URBAN_Openroad', \n",
    "'URBAN_Urban', \n",
    "'LIGHT_Bright Sun', \n",
    "'LIGHT_Dark', \n",
    "'LIGHT_Overcast', \n",
    "'LIGHT_Twilight',  \n",
    "'STREET_LIGHT_None', \n",
    "'STREET_LIGHT_Off', \n",
    "'STREET_LIGHT_On',  \n",
    "'WEATHER_A_Fine', \n",
    "'WEATHER_A_Heavy Rain', \n",
    "'WEATHER_A_Light Rain', \n",
    "'WEATHER_A_Mist', \n",
    "'WEATHER_A_Snow',\n",
    "'CLUSTER_SCALED']\n",
    "\n",
    "df_non_injury = df[df.CRASH_SEV_CODE==2]  # 480452 samples\n",
    "df_minor = df[df.CRASH_SEV_CODE==1]       # 150834 samples\n",
    "df_serious = df[df.CRASH_SEV_CODE==3]    # 37347  samples\n",
    "df_fatal = df[df.CRASH_SEV_CODE==0]       # 6178   samples\n",
    "\n",
    "df_minor_upsampled = resample(df_minor, replace=True,n_samples=480452,random_state=40)\n",
    "df_serious_upsampled = resample(df_serious, replace=True,n_samples=480452,random_state=40)\n",
    "df_fatal_upsampled = resample(df_fatal, replace=True,n_samples=480452,random_state=40)\n",
    "\n",
    "df_upsampled = pd.concat([df_non_injury, df_minor_upsampled, df_serious_upsampled, df_fatal_upsampled])\n",
    "\n",
    "df_upsampled.CRASH_SEV_CODE.value_counts()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1729627, 83)\n",
      "(192181, 83)\n"
     ]
    }
   ],
   "source": [
    "X = df_upsampled.drop([\"CRASH_SEV_CODE\"], axis=1).values\n",
    "y = df_upsampled[\"CRASH_SEV_CODE\"].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=40)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "print(X_train.shape); print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.16832581\n",
      "Iteration 2, loss = 1.07541282\n",
      "Iteration 3, loss = 1.03991732\n",
      "Iteration 4, loss = 1.02180869\n",
      "Iteration 5, loss = 1.01160595\n",
      "Iteration 6, loss = 1.00418975\n",
      "Iteration 7, loss = 0.99830784\n",
      "Iteration 8, loss = 0.99282192\n",
      "Iteration 9, loss = 0.98880303\n",
      "Iteration 10, loss = 0.98537841\n",
      "Iteration 11, loss = 0.98202037\n",
      "Iteration 12, loss = 0.97958597\n",
      "Iteration 13, loss = 0.97715531\n",
      "Iteration 14, loss = 0.97484923\n",
      "Iteration 15, loss = 0.97286178\n",
      "Iteration 16, loss = 0.97071261\n",
      "Iteration 17, loss = 0.96925404\n",
      "Iteration 18, loss = 0.96761022\n",
      "Iteration 19, loss = 0.96593055\n",
      "Iteration 20, loss = 0.96468667\n",
      "Iteration 21, loss = 0.96316838\n",
      "Iteration 22, loss = 0.96212220\n",
      "Iteration 23, loss = 0.96070175\n",
      "Iteration 24, loss = 0.95944361\n",
      "Iteration 25, loss = 0.95865197\n",
      "Iteration 26, loss = 0.95740534\n",
      "Iteration 27, loss = 0.95672822\n",
      "Iteration 28, loss = 0.95582245\n",
      "Iteration 29, loss = 0.95498976\n",
      "Iteration 30, loss = 0.95402917\n",
      "Iteration 31, loss = 0.95320513\n",
      "Iteration 32, loss = 0.95262025\n",
      "Iteration 33, loss = 0.95202115\n",
      "Iteration 34, loss = 0.95108880\n",
      "Iteration 35, loss = 0.95065621\n",
      "Iteration 36, loss = 0.95015180\n",
      "Iteration 37, loss = 0.94959376\n",
      "Iteration 38, loss = 0.94878569\n",
      "Iteration 39, loss = 0.94832739\n",
      "Iteration 40, loss = 0.94767621\n",
      "Iteration 41, loss = 0.94718985\n",
      "Iteration 42, loss = 0.94653819\n",
      "Iteration 43, loss = 0.94622986\n",
      "Iteration 44, loss = 0.94559525\n",
      "Iteration 45, loss = 0.94507430\n",
      "Iteration 46, loss = 0.94480971\n",
      "Iteration 47, loss = 0.94424935\n",
      "Iteration 48, loss = 0.94376239\n",
      "Iteration 49, loss = 0.94325749\n",
      "Iteration 50, loss = 0.94291843\n",
      "Iteration 51, loss = 0.94261720\n",
      "Iteration 52, loss = 0.94191521\n",
      "Iteration 53, loss = 0.94174782\n",
      "Iteration 54, loss = 0.94153957\n",
      "Iteration 55, loss = 0.94093750\n",
      "Iteration 56, loss = 0.94078478\n",
      "Iteration 57, loss = 0.94018846\n",
      "Iteration 58, loss = 0.93981225\n",
      "Iteration 59, loss = 0.93983743\n",
      "Iteration 60, loss = 0.93938847\n",
      "Iteration 61, loss = 0.93920898\n",
      "Iteration 62, loss = 0.93865005\n",
      "Iteration 63, loss = 0.93840397\n",
      "Iteration 64, loss = 0.93830496\n",
      "Iteration 65, loss = 0.93775308\n",
      "Iteration 66, loss = 0.93799393\n",
      "Iteration 67, loss = 0.93721242\n",
      "Iteration 68, loss = 0.93717454\n",
      "Iteration 69, loss = 0.93713316\n",
      "Iteration 70, loss = 0.93666318\n",
      "Iteration 71, loss = 0.93604407\n",
      "Iteration 72, loss = 0.93570396\n",
      "Iteration 73, loss = 0.93567158\n",
      "Iteration 74, loss = 0.93576351\n",
      "Iteration 75, loss = 0.93535068\n",
      "Iteration 76, loss = 0.93497598\n",
      "Iteration 77, loss = 0.93506776\n",
      "Iteration 78, loss = 0.93489147\n",
      "Iteration 79, loss = 0.93460332\n",
      "Iteration 80, loss = 0.93434818\n",
      "Iteration 81, loss = 0.93408626\n",
      "Iteration 82, loss = 0.93378625\n",
      "Iteration 83, loss = 0.93354475\n",
      "Iteration 84, loss = 0.93349064\n",
      "Iteration 85, loss = 0.93321753\n",
      "Iteration 86, loss = 0.93307005\n",
      "Iteration 87, loss = 0.93324980\n",
      "Iteration 88, loss = 0.93253093\n",
      "Iteration 89, loss = 0.93298134\n",
      "Iteration 90, loss = 0.93249521\n",
      "Iteration 91, loss = 0.93246691\n",
      "Iteration 92, loss = 0.93226062\n",
      "Iteration 93, loss = 0.93203503\n",
      "Iteration 94, loss = 0.93155638\n",
      "Iteration 95, loss = 0.93138509\n",
      "Iteration 96, loss = 0.93160945\n",
      "Iteration 97, loss = 0.93151884\n",
      "Iteration 98, loss = 0.93129939\n",
      "Iteration 99, loss = 0.93114947\n",
      "Iteration 100, loss = 0.93124660\n",
      "Iteration 101, loss = 0.93080559\n",
      "Iteration 102, loss = 0.93060141\n",
      "Iteration 103, loss = 0.93054960\n",
      "Iteration 104, loss = 0.93087446\n",
      "Iteration 105, loss = 0.93018545\n",
      "Iteration 106, loss = 0.93003643\n",
      "Iteration 107, loss = 0.92997228\n",
      "Iteration 108, loss = 0.92986737\n",
      "Iteration 109, loss = 0.92964620\n",
      "Iteration 110, loss = 0.92978566\n",
      "Iteration 111, loss = 0.92985959\n",
      "Iteration 112, loss = 0.92939040\n",
      "Iteration 113, loss = 0.92977683\n",
      "Iteration 114, loss = 0.92946908\n",
      "Iteration 115, loss = 0.92921426\n",
      "Iteration 116, loss = 0.92896907\n",
      "Iteration 117, loss = 0.92882608\n",
      "Iteration 118, loss = 0.92887747\n",
      "Iteration 119, loss = 0.92875584\n",
      "Iteration 120, loss = 0.92878928\n",
      "Iteration 121, loss = 0.92852570\n",
      "Iteration 122, loss = 0.92829832\n",
      "Iteration 123, loss = 0.92831972\n",
      "Iteration 124, loss = 0.92802552\n",
      "Iteration 125, loss = 0.92800580\n",
      "Iteration 126, loss = 0.92795133\n",
      "Iteration 127, loss = 0.92758778\n",
      "Iteration 128, loss = 0.92749219\n",
      "Iteration 129, loss = 0.92725603\n",
      "Iteration 130, loss = 0.92738990\n",
      "Iteration 131, loss = 0.92700767\n",
      "Iteration 132, loss = 0.92718876\n",
      "Iteration 133, loss = 0.92721557\n",
      "Iteration 134, loss = 0.92711878\n",
      "Iteration 135, loss = 0.92677183\n",
      "Iteration 136, loss = 0.92717576\n",
      "Iteration 137, loss = 0.92669972\n",
      "Iteration 138, loss = 0.92679890\n",
      "Iteration 139, loss = 0.92658694\n",
      "Iteration 140, loss = 0.92672725\n",
      "Iteration 141, loss = 0.92664795\n",
      "Iteration 142, loss = 0.92677242\n",
      "Iteration 143, loss = 0.92616543\n",
      "Iteration 144, loss = 0.92618747\n",
      "Iteration 145, loss = 0.92639317\n",
      "Iteration 146, loss = 0.92618555\n",
      "Iteration 147, loss = 0.92616721\n",
      "Iteration 148, loss = 0.92593770\n",
      "Iteration 149, loss = 0.92621820\n",
      "Iteration 150, loss = 0.92593176\n",
      "Iteration 151, loss = 0.92565929\n",
      "Iteration 152, loss = 0.92574897\n",
      "Iteration 153, loss = 0.92556978\n",
      "Iteration 154, loss = 0.92606278\n",
      "Iteration 155, loss = 0.92551797\n",
      "Iteration 156, loss = 0.92519870\n",
      "Iteration 157, loss = 0.92556895\n",
      "Iteration 158, loss = 0.92516113\n",
      "Iteration 159, loss = 0.92537548\n",
      "Iteration 160, loss = 0.92520884\n",
      "Iteration 161, loss = 0.92536351\n",
      "Iteration 162, loss = 0.92500952\n",
      "Iteration 163, loss = 0.92493079\n",
      "Iteration 164, loss = 0.92518234\n",
      "Iteration 165, loss = 0.92489217\n",
      "Iteration 166, loss = 0.92524690\n",
      "Iteration 167, loss = 0.92514005\n",
      "Iteration 168, loss = 0.92470622\n",
      "Iteration 169, loss = 0.92497584\n",
      "Iteration 170, loss = 0.92458993\n",
      "Iteration 171, loss = 0.92479581\n",
      "Iteration 172, loss = 0.92471182\n",
      "Iteration 173, loss = 0.92473584\n",
      "Iteration 174, loss = 0.92449671\n",
      "Iteration 175, loss = 0.92443900\n",
      "Iteration 176, loss = 0.92435200\n",
      "Iteration 177, loss = 0.92411858\n",
      "Iteration 178, loss = 0.92441559\n",
      "Iteration 179, loss = 0.92434308\n",
      "Iteration 180, loss = 0.92406827\n",
      "Iteration 181, loss = 0.92432184\n",
      "Iteration 182, loss = 0.92462913\n",
      "Iteration 183, loss = 0.92394680\n",
      "Iteration 184, loss = 0.92414041\n",
      "Iteration 185, loss = 0.92400081\n",
      "Iteration 186, loss = 0.92394362\n",
      "Iteration 187, loss = 0.92411248\n",
      "Iteration 188, loss = 0.92408502\n",
      "Iteration 189, loss = 0.92401334\n",
      "Iteration 190, loss = 0.92403454\n",
      "Iteration 191, loss = 0.92366911\n",
      "Iteration 192, loss = 0.92376063\n",
      "Iteration 193, loss = 0.92385048\n",
      "Iteration 194, loss = 0.92395390\n",
      "Iteration 195, loss = 0.92372204\n",
      "Iteration 196, loss = 0.92337549\n",
      "Iteration 197, loss = 0.92374746\n",
      "Iteration 198, loss = 0.92336498\n",
      "Iteration 199, loss = 0.92328534\n",
      "Iteration 200, loss = 0.92353519\n",
      "Iteration 201, loss = 0.92338038\n",
      "Iteration 202, loss = 0.92312327\n",
      "Iteration 203, loss = 0.92330423\n",
      "Iteration 204, loss = 0.92325685\n",
      "Iteration 205, loss = 0.92315877\n",
      "Iteration 206, loss = 0.92343134\n",
      "Iteration 207, loss = 0.92328807\n",
      "Iteration 208, loss = 0.92354022\n",
      "Iteration 209, loss = 0.92335138\n",
      "Iteration 210, loss = 0.92328734\n",
      "Iteration 211, loss = 0.92299170\n",
      "Iteration 212, loss = 0.92303909\n",
      "Iteration 213, loss = 0.92280415\n",
      "Iteration 214, loss = 0.92259965\n",
      "Iteration 215, loss = 0.92292166\n",
      "Iteration 216, loss = 0.92272834\n",
      "Iteration 217, loss = 0.92295644\n",
      "Iteration 218, loss = 0.92277349\n",
      "Iteration 219, loss = 0.92245673\n",
      "Iteration 220, loss = 0.92259209\n",
      "Iteration 221, loss = 0.92276585\n",
      "Iteration 222, loss = 0.92244185\n",
      "Iteration 223, loss = 0.92249094\n",
      "Iteration 224, loss = 0.92243117\n",
      "Iteration 225, loss = 0.92254033\n",
      "Iteration 226, loss = 0.92248703\n",
      "Iteration 227, loss = 0.92234606\n",
      "Iteration 228, loss = 0.92247911\n",
      "Iteration 229, loss = 0.92253405\n",
      "Iteration 230, loss = 0.92237851\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "\n",
      " Training time: 3367.2456414699554s\n",
      "\n",
      " training data\n",
      "\n",
      "[[430030      0      0   2370]\n",
      " [ 32879  79048  99758 220782]\n",
      " [ 24009  80142 194830 133651]\n",
      " [ 40889  61885  46118 283236]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.90    432400\n",
      "           3       0.44      0.66      0.53    432128\n",
      "           2       0.57      0.45      0.50    432632\n",
      "           1       0.36      0.18      0.24    432467\n",
      "\n",
      "    accuracy                           0.57   1729627\n",
      "   macro avg       0.55      0.57      0.54   1729627\n",
      "weighted avg       0.55      0.57      0.54   1729627\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " test data\n",
      "\n",
      "[[47784     0     0   268]\n",
      " [ 3925  8816 11161 24083]\n",
      " [ 2814  8866 21552 14588]\n",
      " [ 4658  7022  5221 31423]]\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.99      0.89     48052\n",
      "           3       0.45      0.65      0.53     48324\n",
      "           2       0.57      0.45      0.50     47820\n",
      "           1       0.36      0.18      0.24     47985\n",
      "\n",
      "    accuracy                           0.57    192181\n",
      "   macro avg       0.54      0.57      0.54    192181\n",
      "weighted avg       0.54      0.57      0.54    192181\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPClassifier(hidden_layer_sizes=(84,42,1), activation='relu', solver='adam', max_iter=500,verbose = True)\n",
    "\n",
    "start = time.time()\n",
    "mlp.fit(X_train,y_train)\n",
    "stop = time.time()\n",
    "\n",
    "predict_train = mlp.predict(X_train)\n",
    "predict_test = mlp.predict(X_test)\n",
    "print(f\"\\n Training time: {stop - start}s\")\n",
    "print(\"\\n training data\\n\")\n",
    "print(confusion_matrix(y_train,predict_train))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_train,predict_train,labels=pd.unique(predict_train)))\n",
    "print(\"\\n---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"\\n test data\\n\")\n",
    "print(confusion_matrix(y_test,predict_test))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,predict_test,labels=pd.unique(predict_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3]\n"
     ]
    }
   ],
   "source": [
    "classification_report(y_test,predict_test,labels=pd.unique(predict_test))\n",
    "print(mlp.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 8 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "-- Epoch 1-- Epoch 1\n",
      "\n",
      "Norm: 195158.69, NNZs: 18, Bias: 23.000000, T: 1921808, Avg. loss: 686148.332722\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 82456.47, NNZs: 18, Bias: -59.000000, T: 1921808, Avg. loss: 745568.259439\n",
      "Total training time: 0.49 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 116749.24, NNZs: 18, Bias: -29.000000, T: 1921808, Avg. loss: 744053.898060\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 78961.63, NNZs: 18, Bias: 11.000000, T: 1921808, Avg. loss: 672685.401437\n",
      "Total training time: 0.52 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 281231.62, NNZs: 18, Bias: 43.000000, T: 3843616, Avg. loss: 668819.152235\n",
      "Total training time: 0.98 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 128339.92, NNZs: 18, Bias: -118.000000, T: 3843616, Avg. loss: 743692.985429\n",
      "Total training time: 1.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 164033.92, NNZs: 18, Bias: -53.000000, T: 3843616, Avg. loss: 736939.681098\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 94703.29, NNZs: 18, Bias: 21.000000, T: 3843616, Avg. loss: 668351.320158\n",
      "Total training time: 1.02 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 281043.54, NNZs: 18, Bias: 62.000000, T: 5765424, Avg. loss: 663922.374089\n",
      "Total training time: 1.49 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 164845.23, NNZs: 18, Bias: -173.000000, T: 5765424, Avg. loss: 742484.267684\n",
      "Total training time: 1.51 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 99212.75, NNZs: 18, Bias: 26.000000, T: 5765424, Avg. loss: 667427.094227\n",
      "Total training time: 1.52 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 180385.45, NNZs: 18, Bias: -71.000000, T: 5765424, Avg. loss: 735944.742280\n",
      "Total training time: 1.53 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 284448.76, NNZs: 18, Bias: 77.000000, T: 7687232, Avg. loss: 666238.806286\n",
      "Total training time: 1.99 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 193309.21, NNZs: 18, Bias: -229.000000, T: 7687232, Avg. loss: 742634.634004\n",
      "Total training time: 2.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 178591.61, NNZs: 18, Bias: -90.000000, T: 7687232, Avg. loss: 737281.690980\n",
      "Total training time: 2.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 114392.84, NNZs: 18, Bias: 33.000000, T: 7687232, Avg. loss: 667188.410140\n",
      "Total training time: 2.03 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 267040.42, NNZs: 18, Bias: 92.000000, T: 9609040, Avg. loss: 663701.757230\n",
      "Total training time: 2.48 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 221702.21, NNZs: 18, Bias: -281.000000, T: 9609040, Avg. loss: 740707.935020\n",
      "Total training time: 2.50 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 164507.57, NNZs: 18, Bias: -100.000000, T: 9609040, Avg. loss: 735595.959721\n",
      "Total training time: 2.52 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 128078.52, NNZs: 18, Bias: 41.000000, T: 9609040, Avg. loss: 666085.479523\n",
      "Total training time: 2.53 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 302644.18, NNZs: 18, Bias: 110.000000, T: 11530848, Avg. loss: 665623.670007\n",
      "Total training time: 2.98 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 241944.57, NNZs: 18, Bias: -334.000000, T: 11530848, Avg. loss: 740694.201902\n",
      "Total training time: 3.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 140047.04, NNZs: 18, Bias: 50.000000, T: 11530848, Avg. loss: 665486.938070\n",
      "Total training time: 3.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 179042.76, NNZs: 18, Bias: -114.000000, T: 11530848, Avg. loss: 735330.204818\n",
      "Total training time: 3.03 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 314326.77, NNZs: 18, Bias: 127.000000, T: 13452656, Avg. loss: 665739.303313\n",
      "Total training time: 3.47 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 256793.20, NNZs: 18, Bias: -389.000000, T: 13452656, Avg. loss: 741833.595219\n",
      "Total training time: 3.48 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 152221.91, NNZs: 18, Bias: 58.000000, T: 13452656, Avg. loss: 664778.554352\n",
      "Total training time: 3.50 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 182794.35, NNZs: 18, Bias: -126.000000, T: 13452656, Avg. loss: 735416.718362\n",
      "Total training time: 3.53 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 312677.48, NNZs: 18, Bias: 146.000000, T: 15374464, Avg. loss: 664718.807071\n",
      "Total training time: 3.95 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 275984.99, NNZs: 18, Bias: -442.000000, T: 15374464, Avg. loss: 740790.088620\n",
      "Total training time: 3.97 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 164044.51, NNZs: 18, Bias: 70.000000, T: 15374464, Avg. loss: 664870.584721\n",
      "Total training time: 4.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 193448.74, NNZs: 18, Bias: -141.000000, T: 15374464, Avg. loss: 736198.512778\n",
      "Total training time: 4.03 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 340929.51, NNZs: 18, Bias: 164.000000, T: 17296272, Avg. loss: 665342.392998\n",
      "Total training time: 4.46 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 291510.20, NNZs: 18, Bias: -492.000000, T: 17296272, Avg. loss: 740458.046660\n",
      "Total training time: 4.47 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 208870.90, NNZs: 18, Bias: -156.000000, T: 17296272, Avg. loss: 735127.921468\n",
      "Total training time: 4.55 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 169751.45, NNZs: 18, Bias: 77.000000, T: 17296272, Avg. loss: 664690.187989\n",
      "Total training time: 4.55 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 315676.18, NNZs: 18, Bias: 178.000000, T: 19218080, Avg. loss: 663358.579583\n",
      "Total training time: 4.94 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 302416.19, NNZs: 18, Bias: -540.000000, T: 19218080, Avg. loss: 741797.961406\n",
      "Total training time: 4.96 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 175227.74, NNZs: 18, Bias: 92.000000, T: 19218080, Avg. loss: 663635.702170\n",
      "Total training time: 5.05 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 202263.14, NNZs: 18, Bias: -166.000000, T: 19218080, Avg. loss: 736005.228902\n",
      "Total training time: 5.06 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 346449.83, NNZs: 18, Bias: 199.000000, T: 21139888, Avg. loss: 665030.129319\n",
      "Total training time: 5.44 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 312863.75, NNZs: 18, Bias: -591.000000, T: 21139888, Avg. loss: 740571.519970\n",
      "Total training time: 5.47 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 180994.86, NNZs: 18, Bias: 101.000000, T: 21139888, Avg. loss: 664583.727511\n",
      "Total training time: 5.58 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 204938.66, NNZs: 18, Bias: -182.000000, T: 21139888, Avg. loss: 736162.327586\n",
      "Total training time: 5.60 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 317738.09, NNZs: 18, Bias: 213.000000, T: 23061696, Avg. loss: 663940.773949\n",
      "Total training time: 5.93 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 321529.40, NNZs: 18, Bias: -640.000000, T: 23061696, Avg. loss: 740874.798785\n",
      "Total training time: 5.97 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 194419.59, NNZs: 18, Bias: 114.000000, T: 23061696, Avg. loss: 663942.265932\n",
      "Total training time: 6.09 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 212689.47, NNZs: 18, Bias: -199.000000, T: 23061696, Avg. loss: 735096.008495\n",
      "Total training time: 6.10 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 302908.16, NNZs: 18, Bias: 231.000000, T: 24983504, Avg. loss: 663694.587688\n",
      "Total training time: 6.42 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 333625.83, NNZs: 18, Bias: -693.000000, T: 24983504, Avg. loss: 740457.649271\n",
      "Total training time: 6.45 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 192987.60, NNZs: 18, Bias: 125.000000, T: 24983504, Avg. loss: 664458.871953\n",
      "Total training time: 6.59 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 221175.87, NNZs: 18, Bias: -212.000000, T: 24983504, Avg. loss: 734940.608516\n",
      "Total training time: 6.61 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 332653.47, NNZs: 18, Bias: 248.000000, T: 26905312, Avg. loss: 664514.539534\n",
      "Total training time: 6.90 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 339826.06, NNZs: 18, Bias: -743.000000, T: 26905312, Avg. loss: 740865.351133\n",
      "Total training time: 6.94 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 204372.16, NNZs: 18, Bias: 136.000000, T: 26905312, Avg. loss: 663865.339165\n",
      "Total training time: 7.09 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 210676.43, NNZs: 18, Bias: -224.000000, T: 26905312, Avg. loss: 735683.247926\n",
      "Total training time: 7.12 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 333436.39, NNZs: 18, Bias: 263.000000, T: 28827120, Avg. loss: 665603.929677\n",
      "Total training time: 7.40 seconds.\n",
      "Convergence after 15 epochs took 7.40 seconds\n",
      "Norm: 348727.50, NNZs: 18, Bias: -794.000000, T: 28827120, Avg. loss: 740995.184396\n",
      "Total training time: 7.44 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 212273.65, NNZs: 18, Bias: 153.000000, T: 28827120, Avg. loss: 664268.322590\n",
      "Total training time: 7.58 seconds.\n",
      "Convergence after 15 epochs took 7.58 seconds\n",
      "Norm: 240578.95, NNZs: 18, Bias: -241.000000, T: 28827120, Avg. loss: 734738.726650\n",
      "Total training time: 7.59 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 352970.77, NNZs: 18, Bias: -842.000000, T: 30748928, Avg. loss: 741050.593514\n",
      "Total training time: 7.86 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 251035.54, NNZs: 18, Bias: -258.000000, T: 30748928, Avg. loss: 734902.016352\n",
      "Total training time: 7.99 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 359489.97, NNZs: 18, Bias: -893.000000, T: 32670736, Avg. loss: 741931.772918\n",
      "Total training time: 8.25 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 231948.58, NNZs: 18, Bias: -266.000000, T: 32670736, Avg. loss: 735565.659500\n",
      "Total training time: 8.39 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 370116.30, NNZs: 18, Bias: -944.000000, T: 34592544, Avg. loss: 739542.007891\n",
      "Total training time: 8.62 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 218418.23, NNZs: 18, Bias: -280.000000, T: 34592544, Avg. loss: 735763.668320\n",
      "Total training time: 8.80 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 375234.74, NNZs: 18, Bias: -995.000000, T: 36514352, Avg. loss: 741123.716460\n",
      "Total training time: 9.03 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 221729.52, NNZs: 18, Bias: -294.000000, T: 36514352, Avg. loss: 735453.782756\n",
      "Total training time: 9.23 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 382739.10, NNZs: 18, Bias: -1048.000000, T: 38436160, Avg. loss: 741434.340264\n",
      "Total training time: 9.45 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 223720.15, NNZs: 18, Bias: -305.000000, T: 38436160, Avg. loss: 735461.767411\n",
      "Total training time: 9.63 seconds.\n",
      "Convergence after 20 epochs took 9.63 seconds\n",
      "Norm: 387854.89, NNZs: 18, Bias: -1103.000000, T: 40357968, Avg. loss: 741370.908337\n",
      "Total training time: 9.84 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 392933.55, NNZs: 18, Bias: -1154.000000, T: 42279776, Avg. loss: 740334.383444\n",
      "Total training time: 10.24 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 399304.38, NNZs: 18, Bias: -1205.000000, T: 44201584, Avg. loss: 740429.665863\n",
      "Total training time: 10.64 seconds.\n",
      "Convergence after 23 epochs took 10.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   10.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:   10.6s finished\n"
     ]
    }
   ],
   "source": [
    "# only run this cell and below for feature importance\n",
    "from sklearn.linear_model import Perceptron\n",
    "clf = Perceptron(tol=1e-3, random_state=40, verbose=True,n_jobs = -1)\n",
    "clf.fit(X,y)\n",
    "\n",
    "coeffs = clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      " min:  -657.505707130214  feature:  ROAD_SURFACE_Unknown \n",
      " max:  37.03248869219892  feature:  MULTI_VEH_Other\n",
      "-------------------\n",
      "-------------------\n",
      " min:  -1315.1133036655588  feature:  DIRN_ROLE1_DESC_0 \n",
      " max:  657.6182417089478  feature:  ROAD_SURFACE_Unknown\n",
      "-------------------\n",
      "-------------------\n",
      " min:  -657.5057071302133  feature:  ROAD_SURFACE_Unknown \n",
      " max:  1315.2258382442915  feature:  DIRN_ROLE1_DESC_0\n",
      "-------------------\n",
      "-------------------\n",
      " min:  -657.5193937681679  feature:  ROAD_SURFACE_Unknown \n",
      " max:  41.12664954461998  feature:  MULTI_VEH_Cyclist(s)+Pedestrian(s) only\n",
      "-------------------\n",
      "\n",
      " training data\n",
      "\n",
      "[[ 94361 101434  51191 185414]\n",
      " [102238 125102 112274  92853]\n",
      " [ 82660 123299 161536  65137]\n",
      " [ 98839 110351  85673 137265]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.22      0.23    432400\n",
      "           2       0.39      0.37      0.38    432632\n",
      "           1       0.27      0.29      0.28    432467\n",
      "           3       0.29      0.32      0.30    432128\n",
      "\n",
      "    accuracy                           0.30   1729627\n",
      "   macro avg       0.30      0.30      0.30   1729627\n",
      "weighted avg       0.30      0.30      0.30   1729627\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " test data\n",
      "\n",
      "[[10569 11318  5664 20501]\n",
      " [11378 13703 12521 10383]\n",
      " [ 9098 13703 17780  7239]\n",
      " [10968 12436  9603 15317]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      0.22      0.23     48052\n",
      "           2       0.39      0.37      0.38     47820\n",
      "           1       0.27      0.29      0.28     47985\n",
      "           3       0.29      0.32      0.30     48324\n",
      "\n",
      "    accuracy                           0.30    192181\n",
      "   macro avg       0.30      0.30      0.30    192181\n",
      "weighted avg       0.30      0.30      0.30    192181\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df.drop([\"CRASH_SEV_CODE\"], axis=1).columns\n",
    "minV = 0\n",
    "maxV = 0\n",
    "minPlace = 0\n",
    "maxPlace = 0\n",
    "count = 0\n",
    "for i in coeffs:\n",
    "    for j in i:\n",
    "        if count == 0:\n",
    "            minPlace = 0\n",
    "            maxPlace = 0\n",
    "            minV = j\n",
    "            maxV = j\n",
    "        else:\n",
    "            if minV > j:\n",
    "                minV =j\n",
    "                minPlace = count\n",
    "            if maxV < j:\n",
    "                maxV = j\n",
    "                maxPlace = count\n",
    "        count+=1\n",
    "    \n",
    "    count = 0\n",
    "    print(\"-------------------\")\n",
    "    print(\" min: \",minV,\" feature: \",features[minPlace],\"\\n max: \",maxV,\" feature: \",features[maxPlace])\n",
    "    print(\"-------------------\")\n",
    "\n",
    "predict_train2 = clf.predict(X_train)\n",
    "predict_test2 = clf.predict(X_test)\n",
    "\n",
    "print(\"\\n training data\\n\")\n",
    "print(confusion_matrix(y_train,predict_train2))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_train,predict_train2,labels=pd.unique(predict_train)))\n",
    "print(\"\\n---------------------------------------------------------------------------------------\\n\")\n",
    "print(\"\\n test data\\n\")\n",
    "print(confusion_matrix(y_test,predict_test2))\n",
    "print(\"\\n\")\n",
    "print(classification_report(y_test,predict_test2,labels=pd.unique(predict_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
